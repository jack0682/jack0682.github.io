---
title: "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609"
excerpt: "A comprehensive robotic assembly system integrating reinforcement learning, safe motion planning, and real-time perception for autonomous 3D puzzle solving with collaborative robots"
date: 2025-01-24
categories:
  - Robotics
  - Reinforcement Learning  
  - Motion Planning
  - Computer Vision
tags:
  - Deep Q-Network
  - Collaborative Robot
  - Soma Cube Assembly
  - ZYZ Euler Angles
  - Sim-to-Real Transfer
  - Legal Action Masking
  - ROS2
  - Unity Integration
header:
  teaser: /assets/images/somacube-paper/system_architecture.png
toc: true
toc_sticky: true
---

> *"In the intersection of reinforcement learning and classical robotics, we discover not merely automated assembly, but the emergence of spatial intelligence that rivals human intuition."*

### **"Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609"**

**Publication Details:**
- **arXiv ID**: [2508.21272](https://arxiv.org/abs/2508.21272)
- **Submission Date**: Aug 29, 2025
- **Author**: Oh Jaehong, Seungjun Jung, Sawoong Kim
- **Affiliation**: Doosan Robotics Rokey Bootcamp 4th

## Abstract

This comprehensive research presents a groundbreaking approach to autonomous 3D puzzle assembly using a Doosan M0609 collaborative robot equipped with **Legal-Action Masked Deep Q-Network (DQN)** and **Safe ZYZ Regrasp strategy**. The Soma Cube puzzle, consisting of seven distinct polycube pieces requiring assembly into a 3Ã—3Ã—3 cube, serves as an ideal benchmark for evaluating complex spatial reasoning capabilities in robotic manipulation systems.

Our methodology addresses fundamental challenges in learning-based robotic assembly through four revolutionary contributions:

1. **Legal-Action Masking Framework**: Reduces combinatorial action space from 4,536 theoretical combinations to 2,484 physically feasible actions, achieving 26% sample efficiency improvement while maintaining solution completeness
2. **ZYZ Singularity Guard System**: Novel proximity index-based singularity detection preventing computational instabilities with 95.7% success rate in critical configurations  
3. **Integrated Sim-to-Real Pipeline**: Comprehensive domain transfer methodology achieving 91% transfer efficiency from simulation to physical deployment
4. **Multi-Modal Safety Architecture**: Real-time perception integration with Korean speech recognition (94% accuracy) and emergency response systems

**Experimental Validation Results:**
- **Simulation Performance**: 92% assembly success rate with 775.5 average reward over 105,300 training episodes
- **Physical Robot Performance**: 75% end-to-end assembly success rate with Â±1.8mm positional accuracy
- **System Integration**: Complete assembly in 12.3Â±1.8 minutes with robust multi-component coordination
- **Safety Validation**: Zero collision events over 300 trials with comprehensive emergency stop mechanisms

![System Architecture](/assets/images/somacube-paper/system_architecture.png)
*Figure 1: Complete system architecture demonstrating integration of legal-action masked DQN, real-time perception, path planning, and speech recognition components in a unified 4-layer architecture enabling seamless coordination between environment sensing, decision-making, and robot control.*

---

## 1. Introduction & Problem Formulation

### 1.1 Industrial Context and Technological Significance

The convergence of collaborative robotics and artificial intelligence represents one of the most transformative developments in modern manufacturing automation. While traditional industrial robots demonstrate exceptional precision in executing predetermined trajectories for repetitive operations, contemporary collaborative robots (cobots) must exhibit advanced capabilities including **real-time adaptability**, **human-safe interaction protocols**, and **intelligent decision-making processes** when operating in dynamic environments alongside human operators.

Recent breakthroughs in deep reinforcement learning have achieved remarkable success across diverse domains ranging from strategic game-playing systems to high-speed autonomous control and sophisticated robotic manipulation tasks. However, the application of reinforcement learning methodologies to real-world robotic assembly presents several fundamental and interconnected challenges:

**Core Technical Challenges:**

1. **Combinatorial Action Space Explosion**: Multi-component assembly tasks suffer from exponential growth in possible action combinations, creating intractable exploration spaces for traditional learning algorithms
2. **Kinematic Safety Constraints**: Six-degree-of-freedom manipulators must navigate complex joint limit boundaries and kinematic singularities while maintaining collision-free operation
3. **Real-Time Perception Integration**: Dynamic environments demand sub-second sensor fusion and decision-making capabilities for practical deployment
4. **Sim-to-Real Domain Gap**: Bridging the performance gap between idealized simulation environments and unpredictable physical deployment conditions

![Soma Cube Pieces](/assets/images/somacube-paper/soma_cube_pieces.png)
*Figure 2: The seven distinct Soma Cube pieces with comprehensive geometric analysis: Corner (8 unique orientations), Positive (24), Negative (24), Zee (12), Tee (12), Ell (24), Three (12). Each piece contributes uniquely to the combinatorial complexity of the assembly task, requiring sophisticated spatial reasoning and constraint satisfaction.*

### 1.2 Soma Cube as Benchmark Problem

The Soma Cube puzzle, conceptualized by Danish polymath Piet Hein in 1933, consists of seven distinct polycube pieces that must be assembled into a complete 3Ã—3Ã—3 cubic structure. This deceptively simple puzzle presents an ideal benchmark for evaluating intelligent robotic assembly systems due to its unique combination of:

**Complexity Characteristics:**
- **Geometric Constraints**: Each piece occupies 3-4 unit cubes with specific orthogonal connectivity patterns
- **Spatial Reasoning Requirements**: Assembly demands understanding of 3D spatial relationships and interference patterns  
- **Sequential Dependencies**: Placement order significantly impacts solution feasibility
- **Multiple Valid Solutions**: 240 distinct valid assembly configurations exist, requiring flexible solution strategies

**Real-World Relevance:**
Unlike simplified pick-and-place demonstrations, Soma Cube assembly mirrors critical aspects of industrial manufacturing scenarios:
- **Mechanical Component Assembly**: Automotive parts, electronic modules, precision instruments
- **Construction Automation**: Modular building systems, prefabricated structures
- **Electronic Packaging**: Circuit board assembly, component placement, cable routing
- **Pharmaceutical Manufacturing**: Capsule assembly, packaging automation

![Real Robot Deployment](/assets/images/somacube-paper/realrobot_assembly_photo.png)
*Figure 3: Real-world deployment of the reinforcement learning-based Soma Cube assembly system on Doosan M0609 collaborative robot equipped with OnRobot RG2 gripper, Intel RealSense D435i depth camera, and comprehensive safety monitoring systems, demonstrating successful integration of perception, planning, and control components in practical manufacturing environment.*

### 1.3 Mathematical Problem Formulation

#### 1.3.1 Formal Assembly Problem Definition

Let $$\mathcal{P} = \{P_1, P_2, \ldots, P_7\}$$ represent the complete set of seven Soma Cube pieces, where each piece $$P_i$$ occupies $$n_i \in \{3, 4\}$$ unit cube positions within the three-dimensional space. The target assembly space is defined as the discrete 3Ã—3Ã—3 grid:

$$\mathcal{G} = \{(x,y,z) : x,y,z \in \{0,1,2\}\}$$

A **valid assembly configuration** is mathematically represented as a bijective mapping:

$$\phi: \mathcal{P} \rightarrow \mathcal{G} \times \mathcal{O}$$

where $$\mathcal{O}$$ denotes the set of all valid 3D orientations for polycube pieces, subject to the following fundamental constraints:

**Completeness Constraint:**
$$\bigcup_{i=1}^{7} \text{occupied}(\phi(P_i)) = \mathcal{G}$$

**Non-Overlapping Constraint:**
$$\forall i \neq j: \text{occupied}(\phi(P_i)) \cap \text{occupied}(\phi(P_j)) = \emptyset$$

where $$\text{occupied}(\phi(P_i))$$ returns the set of grid positions occupied by piece $$P_i$$ under placement $$\phi(P_i)$$.

#### 1.3.2 Combinatorial Complexity Analysis

The theoretical action space complexity presents significant computational challenges:

**Per-Piece Orientation Analysis:**

| Piece Type | Unique Orientations | Symmetry Group | Complexity Factor |
|------------|-------------------|----------------|-------------------|
| Corner | 8 | $$C_3$$ rotational | Low |
| Positive | 24 | Full orientation space | High |
| Negative | 24 | Full orientation space | High |  
| Zee | 12 | $$D_2$$ dihedral | Medium |
| Tee | 12 | $$D_2$$ dihedral | Medium |
| Ell | 24 | Full orientation space | High |
| Three | 12 | $$D_2$$ dihedral | Medium |

**Total Theoretical Action Space:**

$$|\mathcal{A}_{theoretical}| = \sum_{i=1}^{7} |\mathcal{O}_i| \times |\mathcal{G}| = (8+24+24+12+12+24+12) \times 27 = 116 \times 27 = 3,132$$


$$\
\left\lvert \mathcal{A}_{\mathrm{feasible}} \right\rvert \approx 2484
\$$

executable actions, representing a **21% constraint-based reduction** that forms the foundation of our legal-action masking approach.


### 1.4 Markov Decision Process (MDP) Formulation

We formalize the robotic Soma Cube assembly as a finite-horizon MDP defined by the tuple $$(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$$:

#### 1.4.1 State Space Representation

The state space $$\mathcal{S}$$ encodes complete assembly configuration information as a 36-dimensional vector:

$$s_t = [\mathbf{g}_{3 \times 3 \times 3}, \mathbf{p}_7, r_{placed}, r_{index}] \in \mathbb{R}^{36}$$

**Component Breakdown:**
- $$\mathbf{g}_{3 \times 3 \times 3} \in \{0,1\}^{27}$$: Binary occupancy matrix for 3D assembly grid
- $$\mathbf{p}_7 \in \{0,1\}^7$$: One-hot encoding of currently active piece
- $$r_{placed} \in [0,1]$$: Normalized placement progress indicator  
- $$r_{index} \in [0,1]$$: Sequential ordering progress metric

#### 1.4.2 Action Space and Legal Action Masking

Each action $$a \in \mathcal{A}$$ specifies a complete placement operation:

$$a = (\text{piece\_id}, \text{orientation}, \text{position}) \in \mathcal{P} \times \mathcal{O} \times \mathcal{G}$$

**Legal Action Mask Definition:**
For any state $$s$$, the legal action mask $$\mathcal{M}(s) \subseteq \mathcal{A}$$ contains only physically feasible actions:

$$a \in \mathcal{M}(s) \iff \bigwedge_{c \in \mathcal{C}} c(s,a)$$

where $$\mathcal{C} = \{c_{collision}, c_{support}, c_{reach}, c_{vertical}\}$$ represents the comprehensive constraint set:

**Collision Avoidance:** $$c_{collision}(s,a) = [\text{occupied}(a) \cap \text{occupied}(s) = \emptyset]$$

**Support Constraint:** $$c_{support}(s,a) = [\exists \text{ stable support surface below occupied}(a)]$$

**Reachability Constraint:** $$c_{reach}(s,a) = [\exists q \in \mathcal{Q} : f_k(q) = \text{target\_pose}(a)]$$

**Vertical Access:** $$c_{vertical}(s,a) = [\text{clear vertical approach path above occupied}(a)]$$

#### 1.4.3 Reward Function Design

Our carefully crafted reward function encourages robot-friendly assembly sequences while maintaining exploration capabilities:

$$\mathcal{R}(s,a,s') = \mathcal{R}_{base} + \mathcal{R}_{ground} + \mathcal{R}_{access} + \mathcal{R}_{height} + \mathcal{R}_{logic} + \mathcal{R}_{structure}$$

**Individual Reward Components:**

$$\mathcal{R}_{base} = \begin{cases}
+100 & \text{if complete assembly achieved} \\
+10 & \text{if valid piece placement} \\
-10 & \text{if invalid placement attempt} \\
-5 & \text{otherwise}
\end{cases}$$

$$\mathcal{R}_{ground} = \begin{cases}
+30 & \text{if } \min(z_{\text{occupied}}) = 0 \\
+25 & \text{if consecutive ground pieces} \leq 6 \\
0 & \text{otherwise}
\end{cases}$$

$$\mathcal{R}_{access} = \begin{cases}
+8 & \text{if clear vertical access path} \\
-30 & \text{if blocked access (infeasible for robot)}
\end{cases}$$

$$\mathcal{R}_{height} = -8 \times \max(z_{\text{occupied}})$$

$$\mathcal{R}_{logic} = \begin{cases}
+15 & \text{if } \bar{z}_{\text{current}} \leq \bar{z}_{\text{previous}} \\
-15 & \text{otherwise}
\end{cases}$$

$$\mathcal{R}_{structure} = 2 \times |\text{adjacent\_connections}|$$

**Modified Bellman Optimality Equation:**
With legal action masking, the optimal Q-function satisfies:

$$Q^*(s,a) = \mathcal{R}(s,a) + \gamma \max_{a' \in \mathcal{M}(s')} Q^*(s',a')$$

### 1.5 Research Contributions and Innovation

**This study represents the first comprehensive integration of legal-action masking, ZYZ singularity avoidance, and multi-modal perception for autonomous collaborative robot assembly.**

#### 1.5.1 Primary Technical Innovations

1. **Constraint-Aware Reinforcement Learning**: Integration of physics-based constraints directly into the learning process, reducing exploration space by 26% while maintaining solution optimality

2. **Safe Kinematic Motion Planning**: Novel proximity index methodology for ZYZ singularity detection and systematic regrasp planning, achieving 95.7% success rate in critical configurations

3. **Real-Time Multi-Modal Integration**: Seamless fusion of RGB-D perception, force sensing, speech recognition, and global mapping within a unified ROS2 architecture

4. **Production-Ready Sim-to-Real Pipeline**: Comprehensive domain randomization and safety-first deployment achieving 91% transfer efficiency on physical hardware

#### 1.5.2 Quantitative Performance Achievements

| Performance Metric | Specification | Achieved Result | Improvement |
|-------------------|---------------|----------------|-------------|
| **Assembly Success Rate** | > 70% | 75.0% Â± 4.9% | +39.8 pp over baseline |
| **Positional Accuracy** | Â± 2mm | Â± 1.8mm | Exceeds specification |
| **Completion Time** | < 15 min | 12.3 Â± 1.8 min | 18% faster than target |
| **Safety Performance** | 0 collisions | 0/300 trials | Perfect safety record |
| **Speech Recognition** | > 90% | 94% Korean accuracy | Exceeds multilingual goal |
| **Sample Efficiency** | Baseline comparison | +26% improvement | Significant learning speedup |

---

## 2. Related Work and Technical Background

### 2.1 Reinforcement Learning for Robotic Manipulation

#### 2.1.1 Deep Q-Networks in Discrete Control

Deep Q-Networks (DQNs) revolutionized reinforcement learning by demonstrating human-level performance in complex decision-making tasks. The fundamental DQN approach approximates the optimal action-value function:

$$Q^*(s,a) = \mathbb{E}[R_t + \gamma \max_{a'} Q^*(s_{t+1}, a') | s_t = s, a_t = a]$$

using deep neural networks with experience replay and target network stabilization.

**Seminal Contributions:**
- **Mnih et al. (2015)**: Original DQN achieving superhuman Atari performance
- **Van Hasselt et al. (2016)**: Double DQN addressing overestimation bias
- **Wang et al. (2016)**: Dueling architecture separating state value and advantage estimation
- **Schaul et al. (2015)**: Prioritized experience replay improving sample efficiency

**Limitations in Robotic Applications:**
Traditional DQN approaches face significant challenges when applied to robotic manipulation:

1. **Curse of Dimensionality**: Exponential action space growth with multiple objects and orientations
2. **Sparse Reward Signals**: Limited feedback in complex assembly tasks  
3. **Safety Constraints**: Lack of integration with physical system limitations
4. **Sim-to-Real Transfer**: Performance degradation when transitioning to physical robots

#### 2.1.2 Action Masking and Constraint Integration

Recent advances have explored incorporating domain knowledge through action space restrictions:

**Huang et al. (2022)** introduced masked reinforcement learning for invalid action filtering, demonstrating improved sample efficiency in game-theoretic scenarios. However, their approach focused on rule-based constraints rather than physics-based limitations.

**Nachum et al. (2018)** developed hierarchical reinforcement learning with action abstractions, reducing complexity through temporal decomposition rather than constraint-based filtering.

**Our Innovation**: We extend action masking to physics-aware constraint satisfaction, directly integrating kinematic limits, collision avoidance, and manipulation feasibility into the learning process.

### 2.2 Robotic Assembly and Puzzle Solving

#### 2.2.1 Classical Assembly Planning

Traditional assembly planning approaches rely on geometric reasoning and symbolic planning:

**Bertram et al. (2008)** developed geometric constraint satisfaction for polycube puzzles, achieving optimal solutions through systematic search but lacking integration with robotic execution constraints.

**Li et al. (2019)** explored learning-based approaches for 2D puzzle assembly, demonstrating neural network effectiveness but limited to planar configurations without physical manipulation considerations.

**Gap in Current Research**: Existing approaches typically separate logical puzzle solving from physical manipulation planning, leading to solutions that may be geometrically valid but robotically infeasible.

#### 2.2.2 6-DOF Manipulation Planning

Six-degree-of-freedom robotic manipulation presents unique challenges requiring sophisticated path planning methodologies:

**Kinematic Singularities**: Configurations where the robot Jacobian becomes singular, causing:
- Infinite joint velocities for small Cartesian motions  
- Loss of controllability in specific directions
- Potential mechanical damage from excessive joint speeds

**Joint Limit Handling**: Each robot joint $$j$$ operates within bounds $$q_j^{\min} \leq q_j \leq q_j^{\max}$$, requiring path planning algorithms to maintain feasible configurations throughout motion execution.

### 2.3 ZYZ Euler Angle Parameterization

#### 2.3.1 Mathematical Representation

The ZYZ Euler angle convention represents 3D rotations through sequential rotations about Z-Y-Z axes:

$$\mathbf{R}(\alpha, \beta, \gamma) = \mathbf{R}_z(\alpha)\mathbf{R}_y(\beta)\mathbf{R}_z(\gamma)$$

$$= \begin{bmatrix}
\cos\alpha\cos\beta\cos\gamma - \sin\alpha\sin\gamma & -\cos\alpha\cos\beta\sin\gamma - \sin\alpha\cos\gamma & \cos\alpha\sin\beta \\
\sin\alpha\cos\beta\cos\gamma + \cos\alpha\sin\gamma & -\sin\alpha\cos\beta\sin\gamma + \cos\alpha\cos\gamma & \sin\alpha\sin\beta \\
-\sin\beta\cos\gamma & \sin\beta\sin\gamma & \cos\beta
\end{bmatrix}$$

#### 2.3.2 Singularity Analysis

**Gimbal Lock Conditions**: Singularities occur when $$\beta = 0$$ or $$\beta = \pi$$, causing:
- Loss of one degree of rotational freedom
- Infinite solutions for $$(\alpha, \gamma)$$ combinations
- Numerical instabilities in inverse kinematic calculations

**Traditional Mitigation Approaches**:
1. **Quaternion Representation**: Avoids Euler angle singularities but lacks intuitive interpretation
2. **Multiple Representations**: Switch between different Euler conventions based on proximity to singularities
3. **Trajectory Modification**: Plan paths avoiding singular regions

**Our Novel Contribution**: Proximity index-based detection with systematic regrasp planning maintaining ZYZ representation benefits while ensuring computational stability.

### 2.4 Human-Robot Interaction and Speech Recognition

#### 2.4.1 Multilingual Speech Recognition

Recent advances in automatic speech recognition, particularly OpenAI's Whisper architecture, have achieved remarkable accuracy across multiple languages:

**Whisper Model Architecture**:
- **Encoder-Decoder Transformer**: 99M to 1.5B parameter variants
- **Multilingual Training**: 680,000 hours of diverse audio data
- **Robust Performance**: Handles noise, accents, and domain variations

**Korean Language Challenges**:
- **Morphological Complexity**: Extensive agglutination and honorific systems
- **Phonological Variations**: Regional dialects and formal/informal speech patterns  
- **Industrial Noise**: Manufacturing environments with high SNR requirements

#### 2.4.2 Safety-Critical HRI Systems

Human-robot collaboration in industrial environments demands comprehensive safety protocols:

**International Standards**:
- **ISO 10218**: Safety requirements for industrial robots
- **ISO/TS 15066**: Collaborative robot safety specifications  
- **IEC 62061**: Functional safety of electrical/electronic systems

**Implementation Requirements**:
- Emergency stop response time < 100ms
- Force limitation for collision safety
- Workspace monitoring and intrusion detection
- Fail-safe system behavior under all conditions

### 2.5 Sim-to-Real Transfer in Robotics

#### 2.5.1 Domain Randomization Techniques

Effective sim-to-real transfer requires systematic treatment of environmental variations:

**Physical Parameter Randomization**:
- **Mass and Inertia**: Â±10-20% variation in object properties
- **Friction Coefficients**: Surface interaction modeling
- **Actuator Dynamics**: Motor response characteristics and delays

**Sensor Randomization**:
- **Camera Parameters**: Intrinsic calibration variations
- **Depth Noise**: Gaussian noise modeling sensor limitations  
- **Lighting Conditions**: Illumination intensity and color temperature

**Tobin et al. (2017)** demonstrated substantial improvements in real-world robot performance through comprehensive domain randomization, achieving robust transfer across various manipulation tasks.

#### 2.5.2 Gap Analysis in Current Literature

**Identified Research Gaps**:

1. **Limited Integration Studies**: Most research focuses on individual components rather than complete system integration
2. **Simplified Environments**: Controlled laboratory conditions rather than industrial deployment scenarios  
3. **Safety Considerations**: Insufficient attention to safety-critical system requirements
4. **Scalability Questions**: Unclear scaling properties for complex multi-component assemblies

**Our Systematic Approach**: This research addresses these gaps through comprehensive system integration, real-world deployment validation, safety-first design principles, and systematic scalability analysis.

---

## 3. Methodology: Legal-Action Masked DQN Architecture

### 3.1 Hierarchical Deep Q-Network Design

Our legal-action masked DQN architecture employs a sophisticated hierarchical approach that decomposes the complex combinatorial action space into manageable, independently optimizable subspaces. This design philosophy draws inspiration from both dueling network architectures and factorized action representations while introducing novel constraint-aware masking mechanisms.

#### 3.1.1 Network Architecture and Mathematical Foundation

The core DQN architecture utilizes a multi-head design optimized for discrete 3D assembly tasks. The state representation $$s \in \mathbb{R}^{36}$$ encodes complete assembly configuration:

$$s = [\mathbf{g}_{3 \times 3 \times 3}, \mathbf{p}_7, r_{placed}, r_{index}] \in \mathbb{R}^{36}$$

**Feature Extraction Layer:**
The initial feature extraction employs a fully connected layer with ReLU activation:

$$\phi(s) = \text{ReLU}(\mathbf{W}_1 s + \mathbf{b}_1)$$

where $$\mathbf{W}_1 \in \mathbb{R}^{512 \times 36}$$ and $$\mathbf{b}_1 \in \mathbb{R}^{512}$$.

**Hierarchical Processing Layers:**
$$h_1 = \text{ReLU}(\mathbf{W}_2 \phi(s) + \mathbf{b}_2) \in \mathbb{R}^{256}$$
$$h_2 = \text{Dropout}(\text{ReLU}(\mathbf{W}_3 h_1 + \mathbf{b}_3), p=0.3) \in \mathbb{R}^{128}$$

**Dual-Head Architecture:**
The network separates orientation and position value estimation:

$$Q_{\text{orientation}}(s, o) = \mathbf{W}_4^{(o)} h_2 + \mathbf{b}_4^{(o)} \in \mathbb{R}^{|\mathcal{O}|}$$
$$Q_{\text{position}}(s, p) = \mathbf{W}_5^{(p)} h_2 + \mathbf{b}_5^{(p)} \in \mathbb{R}^{27}$$

**Action Value Composition:**
The final Q-value combines orientation and position estimates through additive composition:

$$Q(s, (o,p)) = Q_{\text{orientation}}(s, o) + Q_{\text{position}}(s, p)$$

#### 3.1.2 Theoretical Justification for Additive Decomposition

**Independence Assumption:** The additive decomposition assumes conditional independence of orientation and position preferences given the current state:

$$P(\text{optimal action} = (o^*, p^*) | s) \approx P(\text{optimal orientation} = o^* | s) \cdot P(\text{optimal position} = p^* | s)$$

This assumption holds for Soma Cube assembly where:
- **Orientation Selection**: Primarily constrained by geometric compatibility and piece symmetries
- **Position Selection**: Primarily constrained by spatial occupancy and support requirements

**Computational Efficiency Gain:**
The decomposition reduces network complexity from $$O(|\mathcal{O}| \times |\mathcal{G}|) = O(116 \times 27) = O(3,132)$$ parameters to $$O(|\mathcal{O}|) + O(|\mathcal{G}|) = O(116) + O(27) = O(143)$$ parameters, achieving a **22Ã— reduction** in computational complexity.

### 3.2 Legal Action Masking: Constraint-Aware Learning

![Legal Action Masking](/assets/images/somacube-paper/legal_action_masking.png)
*Figure 4: Legal action masking visualization demonstrating constraint-based action filtering process. (a) Complete theoretical action space containing 3,132 possible action combinations, (b) Physics-based constraint filtering systematically removing collision violations and reachability constraints, (c) Final masked action set containing only 2,484 physically feasible actions, resulting in 26% improvement in learning efficiency and guaranteed solution completeness.*

#### 3.2.1 Constraint Formalization and Implementation

At each decision time step $$t$$, we compute a comprehensive binary mask 
$$
\mathcal{M}(s_t) \in \{0,1\}^{\lvert \mathcal{A} \rvert}
$$ 
representing the legality of each possible action. 

The masked Q-value computation becomes:


$$Q_{\text{masked}}(s,a) = \begin{cases}
Q(s,a) & \text{if } \mathcal{M}(s)[a] = 1 \\
-\infty & \text{if } \mathcal{M}(s)[a] = 0
\end{cases}$$

**Constraint Conjunction:** The legality mask is determined through logical conjunction of all constraint predicates:

$$\mathcal{M}(s)[a] = \bigwedge_{c \in \mathcal{C}} c(s,a) = c_{\text{collision}}(s,a) \land c_{\text{support}}(s,a) \land c_{\text{reach}}(s,a) \land c_{\text{vertical}}(s,a)$$

#### 3.2.2 Individual Constraint Implementations

**1. Collision Avoidance Constraint:**
$$c_{\text{collision}}(s,a): \text{occupied}_{\text{new}}(a) \cap \text{occupied}_{\text{existing}}(s) = \emptyset$$

Implementation uses efficient voxel-based occupancy checking with O(1) lookup time through pre-computed hash tables.

**2. Support Stability Constraint:**
$$c_{\text{support}}(s,a): \exists \text{ stable foundation below } \text{occupied}_{\text{new}}(a)$$

Checks for gravitational support through either:
- Contact with table surface ($$z_{\min} = 0$$)
- Contact with previously placed piece providing stable support

**3. Kinematic Reachability Constraint:**
$$c_{\text{reach}}(s,a): \exists \mathbf{q} \in \mathcal{Q}_{\text{feasible}} : \text{FK}(\mathbf{q}) = \text{target\_pose}(a)$$

Validates inverse kinematics solvability within joint limits:
$$\mathbf{q}_{\min} \leq \mathbf{q} \leq \mathbf{q}_{\max}$$

Uses numerical IK solver with convergence tolerance $$\epsilon = 10^{-6}$$ and maximum iteration limit of 100.

**4. Vertical Access Constraint:**
$$c_{\text{vertical}}(s,a): \text{clear\_path\_from\_above}(\text{target\_position}(a), s) = \text{True}$$

Ensures robot gripper can approach target position from above without collision with existing pieces or workspace boundaries.

#### 3.2.3 Solution Completeness Guarantee

**Theorem (Solution Completeness):** The legal action masking preserves solution completeness under physical robot constraints.

**Formal Statement:** For any valid Soma Cube solution $$\pi^* = (s_0, a_0^*, s_1, a_1^*, \ldots, s_T^*)$$ that is physically executable by the robotic system, we guarantee:

$$\forall t \in [0, T]: a_t^* \in \mathcal{A}_{\text{legal}}(s_t)$$

**Proof Sketch:** 
1. **Physical Executability Assumption**: If solution $$\pi^*$$ is physically executable, then each action $$a_t^*$$ must satisfy all physical constraints by definition
2. **Constraint Completeness**: Our constraint set $$\mathcal{C}$$ captures all fundamental physical limitations (collision, support, reachability, access)
3. **Constraint Consistency**: Each constraint predicate $$c \in \mathcal{C}$$ correctly identifies physical feasibility
4. **Therefore**: $$\pi^*$$ physically executable $$\Rightarrow$$ $$\forall t: \bigwedge_{c \in \mathcal{C}} c(s_t, a_t^*) = \text{True}$$ $$\Rightarrow$$ $$\forall t: a_t^* \in \mathcal{A}_{\text{legal}}(s_t)$$ â–¡

### 3.3 Training Algorithm: Constrained Deep Q-Learning

#### 3.3.1 Modified DQN Update Rule

The legal-action masking requires modification of the standard DQN Bellman target computation. For each sampled transition $$(s, a, r, s', \text{done})$$ from experience replay buffer $$\mathcal{D}$$:

**Standard DQN Target:**
$$y_{\text{standard}} = r + \gamma (1 - \text{done}) \max_{a'} Q_{\theta^-}(s', a')$$

**Legal-Action Masked Target:**
$$y_{\text{masked}} = r + \gamma (1 - \text{done}) \max_{a' \in \mathcal{M}(s')} Q_{\theta^-}(s', a')$$

**Loss Function:**
$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s',\text{done}) \sim \mathcal{D}}\left[(Q_\theta(s,a) - y_{\text{masked}})^2\right]$$

#### 3.3.2 Comprehensive Training Algorithm

**Algorithm 1: Legal-Action Masked DQN Training**

```
Input: Environment â„°, Buffer ð’Ÿ, Q-network Q_Î¸, Target Q_Î¸â», Episodes E
Output: Trained policy Ï€

1:  Initialize replay buffer ð’Ÿ â† âˆ…
2:  Initialize Q_Î¸ with Xavier initialization
3:  Initialize target network Q_Î¸â» â† Q_Î¸  
4:  Initialize hyperparameters: Î±=10â»â´, Î³=0.99, Îµâ‚€=0.9, Îµ_f=0.05
5:  
6:  for episode e = 1 to E do
7:      Reset environment: sâ‚€ â† â„°.reset()
8:      Set episode length: T â† min(1000, âˆž)
9:      
10:     for step t = 0 to T-1 do
11:         // Legal action computation with caching
12:         ð’œ_legal(sâ‚œ) â† GetLegalActions(sâ‚œ)  
13:         if |ð’œ_legal(sâ‚œ)| = 0 then
14:             break  // Terminal state reached
15:         
16:         // Masked Q-value computation  
17:         for a âˆˆ ð’œ do
18:             Q_masked(sâ‚œ,a) â† {Q_Î¸(sâ‚œ,a) if a âˆˆ ð’œ_legal(sâ‚œ); -âˆž otherwise}
19:         
20:         // Epsilon-greedy action selection
21:         Îµâ‚œ â† max(Îµ_f, Îµâ‚€ Â· (decay_rate)áµ—)
22:         if random() < Îµâ‚œ then
23:             aâ‚œ â† random_choice(ð’œ_legal(sâ‚œ))
24:         else
25:             aâ‚œ â† argmax_a Q_masked(sâ‚œ,a)
26:         
27:         // Environment interaction
28:         (sâ‚œâ‚Šâ‚, râ‚œ, done) â† â„°.step(aâ‚œ)
29:         
30:         // Experience storage with priority
31:         priority â† |râ‚œ + Î³ max_{a'âˆˆð’œ_legal(sâ‚œâ‚Šâ‚)} Q_Î¸â»(sâ‚œâ‚Šâ‚,a') - Q_Î¸(sâ‚œ,aâ‚œ)|
32:         ð’Ÿ.store((sâ‚œ, aâ‚œ, râ‚œ, sâ‚œâ‚Šâ‚, done), priority)
33:         
34:         // Training update
35:         if |ð’Ÿ| â‰¥ batch_size and t mod train_freq = 0 then
36:             // Sample prioritized batch
37:             {(sáµ¢, aáµ¢, ráµ¢, s'áµ¢, dáµ¢, wáµ¢)}áµ¢â‚Œâ‚á´® â† ð’Ÿ.sample_prioritized(batch_size)
38:             
39:             // Compute masked targets
40:             for i = 1 to B do
41:                 ð’œ_legal(s'áµ¢) â† GetLegalActions(s'áµ¢)
42:                 yáµ¢ â† ráµ¢ + Î³(1-dáµ¢) max_{a'âˆˆð’œ_legal(s'áµ¢)} Q_Î¸â»(s'áµ¢, a')
43:             
44:             // Weighted loss computation
45:             â„’ â† (1/B) Î£áµ¢â‚Œâ‚á´® wáµ¢(Q_Î¸(sáµ¢,aáµ¢) - yáµ¢)Â²
46:             
47:             // Gradient update with clipping
48:             Î¸ â† Î¸ - Î± Â· clip(âˆ‡_Î¸ â„’, [-1, 1])
49:         
50:         // Target network update
51:         if t mod target_update_freq = 0 then
52:             Q_Î¸â» â† Q_Î¸
53:         
54:         // State transition
55:         sâ‚œ â† sâ‚œâ‚Šâ‚
56:         if done then break
57:     
58:     // Curriculum learning adjustment
59:     if episode_success_rate > 0.8 then
60:         increase_difficulty()
61:
62:  return Ï€(s) = argmax_{aâˆˆð’œ_legal(s)} Q_Î¸(s,a)
```

#### 3.3.3 Hyperparameter Optimization

**Training Configuration:**

| Parameter | Value | Justification |
|-----------|--------|---------------|
| **Learning Rate Î±** | $$10^{-4}$$ | Stable convergence for discrete action spaces |
| **Discount Factor Î³** | 0.99 | Long-term reward consideration for sequential assembly |
| **Replay Buffer Size** | 50,000 | Balance memory efficiency with sample diversity |
| **Batch Size** | 128 | GPU memory optimization for 36D state space |
| **Target Update Frequency** | 20 episodes | Stability vs. learning speed trade-off |
| **Epsilon Decay** | Exponential: $$\epsilon_t = 0.05 + (0.9-0.05)e^{-t/10000}$$ | Exploration-exploitation balance |
| **Dropout Rate** | 0.3 | Regularization for 128-dimensional hidden layers |

**Curriculum Learning Schedule:**

| Level | Pieces | Target Success Rate | Training Episodes |
|-------|--------|-------------------|------------------|
| **Level 1** | 2 pieces | > 95% | 500-1,000 |
| **Level 2** | 3 pieces | > 90% | 1,000-2,000 |  
| **Level 3** | 7 pieces (full) | > 40% | 100,000+ |

### 3.4 Reward Engineering for Robot-Friendly Assembly

#### 3.4.1 Multi-Objective Reward Design Philosophy

Our reward function design follows three fundamental principles:

1. **Robot Kinematic Compatibility**: Encourage assembly sequences that respect joint limits and avoid singularities
2. **Physical Stability**: Prioritize gravitationally stable and mechanically sound configurations
3. **Exploration Maintenance**: Provide sufficient reward shaping without eliminating beneficial exploration

#### 3.4.2 Detailed Reward Component Analysis

**Base Assembly Rewards:**
$$\mathcal{R}_{\text{base}}(s,a,s') = \begin{cases}
+100 & \text{if complete 7-piece assembly achieved} \\
+10 & \text{if valid piece placement executed} \\
+\alpha \cdot \text{density\_increase}(a) & \text{for improved spatial efficiency} \\
-\lambda \cdot \text{collision\_penalty}(a) & \text{for constraint violations} \\
-5 & \text{for no-progress actions}
\end{cases}$$

**Ground-First Assembly Incentive:**
$$\mathcal{R}_{\text{ground}}(s,a,s') = \begin{cases}
+30 & \text{if placed piece touches ground (}z_{\min} = 0\text{)} \\
+25 & \text{if maintaining contiguous ground layer} \\
+15 & \text{if extending ground-connected structure} \\
0 & \text{otherwise}
\end{cases}$$

**Vertical Accessibility Reward:**
$$\mathcal{R}_{\text{access}}(s,a,s') = \begin{cases}
+8 & \text{if clear vertical approach path maintained} \\
-30 & \text{if placement blocks future vertical access} \\
-20 & \text{if requires complex regrasp maneuvers}
\end{cases}$$

**Height Penalty for Stability:**
$$\mathcal{R}_{\text{height}}(s,a,s') = -8 \times \max(z_{\text{occupied}})$$

This linear height penalty encourages bottom-up assembly, reducing mechanical instability and simplifying manipulation.

**Sequential Logic Reward:**
$$\mathcal{R}_{\text{logic}}(s,a,s') = \begin{cases}
+15 & \text{if } \bar{z}_{\text{current}} \leq \bar{z}_{\text{previous}} \\
-15 & \text{if violating bottom-up principle} \\
+5 & \text{if maintaining assembly connectivity}
\end{cases}$$

**Structural Cohesion Bonus:**
$$\mathcal{R}_{\text{structure}}(s,a,s') = 2 \times |\text{adjacent\_face\_connections}|$$

Rewards adjacent cube face contact, encouraging mechanically stable configurations.

#### 3.4.3 Reward Function Validation

**Convergence Analysis:** The total reward function satisfies the Bellman optimality conditions under the constraint that $$\gamma \in (0,1)$$ and all reward components are bounded:

$$|\mathcal{R}(s,a,s')| \leq R_{\max} = 100 + 30 + 8 + 15 + 14 = 167$$

**Empirical Validation Results:**

| Reward Component | Weight | Impact on Success Rate | Impact on Assembly Time |
|------------------|--------|----------------------|------------------------|
| **Base Rewards** | 1.0 | +45% | Baseline |
| **Ground Incentive** | 1.0 | +18% | -25% (faster) |
| **Access Penalty** | 1.0 | +12% | -15% (fewer regrasps) |
| **Height Penalty** | 0.8 | +8% | -10% (simpler motions) |
| **Logic Reward** | 1.0 | +6% | -12% (better sequences) |
| **Structure Bonus** | 0.5 | +3% | +5% (minor overhead) |

**Combined Effect**: Integrated reward function achieves 75% success rate compared to 35% baseline sparse reward, with 18% reduction in average assembly time.

---

## 4. ZYZ Singularity Guard and Safe Path Planning

### 4.1 ZYZ Euler Angle Analysis and Singularity Characterization

#### 4.1.1 Mathematical Foundation of ZYZ Representation

The ZYZ Euler angle convention represents 3D rotations through sequential rotations about the Z-Y-Z axes, providing intuitive interpretation for robotic applications while introducing computational challenges at specific configurations.

**Rotation Matrix Formulation:**
$$\mathbf{R}(\alpha, \beta, \gamma) = \mathbf{R}_z(\alpha)\mathbf{R}_y(\beta)\mathbf{R}_z(\gamma)$$

**Expanded Matrix Form:**
$$\mathbf{R}(\alpha, \beta, \gamma) = \begin{bmatrix}
c_\alpha c_\beta c_\gamma - s_\alpha s_\gamma & -c_\alpha c_\beta s_\gamma - s_\alpha c_\gamma & c_\alpha s_\beta \\
s_\alpha c_\beta c_\gamma + c_\alpha s_\gamma & -s_\alpha c_\beta s_\gamma + c_\alpha c_\gamma & s_\alpha s_\beta \\
-s_\beta c_\gamma & s_\beta s_\gamma & c_\beta
\end{bmatrix}$$

where $$c_\theta = \cos(\theta)$$ and $$s_\theta = \sin(\theta)$$.

#### 4.1.2 Singularity Conditions and Computational Impact

**Gimbal Lock Phenomenon:** Singularities occur when $$\beta = 0Â°$$ or $$\beta = Â±180Â°$$, causing:

1. **Rotational DOF Loss**: The first and third rotation axes become parallel, eliminating one degree of rotational freedom
2. **Parameter Indeterminacy**: Infinite solutions exist for $$(\alpha, \gamma)$$ combinations satisfying $$\alpha + \gamma = \text{constant}$$
3. **Numerical Instability**: Small changes in target orientation near singularities can cause large joint velocities

**Jacobian Analysis Near Singularities:**
The angular velocity mapping near singularities exhibits conditioning problems:

$$\boldsymbol{\omega} = \mathbf{J}(\alpha, \beta, \gamma) \begin{bmatrix} \dot{\alpha} \\ \dot{\beta} \\ \dot{\gamma} \end{bmatrix}$$

where the Jacobian $$\mathbf{J}$$ becomes singular when $$\sin(\beta) \rightarrow 0$$.

**Condition Number Analysis:**
$$\kappa(\mathbf{J}) = \frac{\sigma_{\max}(\mathbf{J})}{\sigma_{\min}(\mathbf{J})} \rightarrow \infty \text{ as } |\beta| \rightarrow 0Â°$$

### 4.2 Proximity Index for Singularity Detection

#### 4.2.1 Novel Proximity Index Formulation

We introduce a **proximity index** (PI) to quantify nearness to singular configurations:

$$\text{PI}(\beta) = 1 - |\cos(\beta)|$$

**Key Properties:**
- $$\text{PI}(\beta) \in [0, 1]$$
- $$\text{PI}(\beta) = 0$$ when $$\beta = 0Â°, 180Â°$$ (singular configurations)  
- $$\text{PI}(\beta) = 1$$ when $$\beta = Â±90Â°$$ (maximally stable configurations)
- Monotonic relationship with singularity distance

**Threshold-Based Detection:**
$$\text{SingularityRisk}(\beta) = \begin{cases}
\text{HIGH} & \text{if } \text{PI}(\beta) < 0.1 \\
\text{MEDIUM} & \text{if } 0.1 \leq \text{PI}(\beta) < 0.3 \\
\text{LOW} & \text{if } \text{PI}(\beta) \geq 0.3
\end{cases}$$

#### 4.2.2 Predictive Singularity Avoidance

**Trajectory Safety Assessment:**
For a planned motion from current orientation $$\mathbf{R}_{\text{current}}$$ to target $$\mathbf{R}_{\text{target}}$$:

1. **Extract Intermediate ZYZ Angles**: Compute ZYZ representation at multiple trajectory points
2. **Evaluate Proximity Index**: Calculate $$\text{PI}(\beta_i)$$ for each intermediate configuration
3. **Safety Classification**: Classify trajectory based on minimum proximity index

$$\text{TrajectorySafety} = \min_{i \in \text{trajectory}} \text{PI}(\beta_i)$$

**Proactive Intervention Criteria:**
- **Safe Execution**: $$\text{TrajectorySafety} > 0.3$$ â†’ Direct motion
- **Caution Required**: $$0.1 < \text{TrajectorySafety} \leq 0.3$$ â†’ Velocity reduction + monitoring  
- **Regrasp Mandatory**: $$\text{TrajectorySafety} \leq 0.1$$ â†’ Activate systematic regrasp sequence

### 4.3 Safe ZYZ Regrasp Algorithm

#### 4.3.1 Systematic Regrasp Planning Strategy

When direct motion would traverse singular regions, our algorithm decomposes the desired rotation into safe, sequential motions with intermediate regrasps.

**Algorithm 2: Safe ZYZ Regrasp with Singularity Avoidance**

```
Input: Target pose T_target, Current pose T_current, Safety threshold Îµ = 0.1
Output: Safe motion sequence M

1:  Extract target ZYZ: (Î±_t, Î²_t, Î³_t) â† ExtractZYZ(T_target)
2:  Compute proximity index: PI(Î²_t) â† 1 - |cos(Î²_t)|
3:  
4:  // Singularity guard activation
5:  if PI(Î²_t) < Îµ or |Î²_t Â± 90Â°| < Îµ then
6:      Î²_t â† sign(Î²_t) Ã— 89.9Â°  // Clamp to safe region
7:  
8:  // Initialize motion sequence
9:  M â† []
10: 
11: // Calculate rotation difference
12: Î”R â† T_target.R Â· T_current.R^(-1)
13: (kÌ‚, Î¸) â† AxisAngleDecomposition(Î”R)
14: 
15: // Assess rotation magnitude and singularity risk
16: if |Î¸| > Î¸_max OR TrajectoryPI(T_current â†’ T_target) < Îµ then
17:     // Decompose dangerous rotation
18:     M.append(MOVE_TO_REGRASP_POSITION)
19:     M.append(OPEN_GRIPPER)
20:     
21:     // Split rotation into safe increments
22:     n_steps â† ceil(|Î¸| / Î¸_max)
23:     for i = 1 to n_steps do
24:         Î¸_i â† (i / n_steps) Ã— Î¸
25:         R_i â† AxisAngleToMatrix(kÌ‚, Î¸_i)
26:         M.append(ROTATE_TO(R_i))
27:         if i < n_steps then
28:             M.append(CLOSE_GRIPPER)
29:             M.append(OPEN_GRIPPER)  // Intermediate regrasp
30:     
31:     M.append(CLOSE_GRIPPER)
32: else
33:     // Safe direct rotation
34:     M.append(ROTATE_DIRECTLY(Î”R))
35: 
36: // Joint limit safety verification
37: for each motion m âˆˆ M do
38:     q â† InverseKinematics(m.pose)
39:     if âˆƒj: q_j âˆ‰ [q_{j,min} + Î´, q_{j,max} - Î´] then
40:         M.insert_before(m, MOVE_TO_SAFE_CONFIGURATION)
41: 
42: return M
```

#### 4.3.2 Regrasp Minimization Strategy

**Optimization Objective:** Minimize the number of regrasp operations while maintaining safety:

$$\min_{M} |\{m \in M : m = \text{REGRASP}\}|$$

subject to:
- $$\forall m \in M: \text{PI}(\text{ZYZ}(m)) \geq \epsilon_{\text{safety}}$$
- $$\forall m \in M: \mathbf{q}_{\min} \leq \text{IK}(m) \leq \mathbf{q}_{\max}$$
- $$\forall m \in M: \text{CollisionFree}(m) = \text{True}$$

**Heuristic Solution Approach:**

1. **Greedy Angle Decomposition**: Split large rotations ($$|\theta| > 90Â°$$) into maximum safe increments
2. **Intermediate Configuration Planning**: Select regrasp poses that minimize subsequent motion complexity  
3. **Joint Space Optimization**: Verify each planned motion maintains joint space feasibility

#### 4.3.3 Performance Analysis and Validation

**Computational Complexity:**
- **Direct Motion Planning**: $$O(1)$$ for safe trajectories
- **Regrasp Sequence Planning**: $$O(n)$$ where $$n = \lceil|\theta|/\theta_{\max}\rceil$$
- **Joint Limit Verification**: $$O(|M|)$$ per planned sequence

**Experimental Validation Results:**

| Test Scenario | Success Rate (%) | Avg. Regrasps | Execution Time (s) |
|---------------|------------------|----------------|-------------------|
| **No Singularity Guard** | 54.3 | N/A | 12.7 |
| **With Proximity Index** | **96.1** | 2.3 | **8.3** |
| **Baseline (Quaternion)** | 89.2 | 0 | 6.1 |

**Key Performance Insights:**
- **95.7% success rate** in singularity-prone configurations (vs. 54% without guard)
- **35% reduction** in unnecessary motions through intelligent regrasp planning
- **67% improvement** in path smoothness (jerk reduction from 15.2 to 8.7 rad/sÂ³)

### 4.4 Integration with Motion Planning Pipeline

#### 4.4.1 Hierarchical Motion Planning Architecture

The ZYZ singularity guard integrates seamlessly with the broader motion planning system through a hierarchical control structure:

**Level 1: Task Planning**
- DQN policy selects next piece and target placement
- High-level assembly sequence optimization

**Level 2: Path Planning**  
- A* search for collision-free Cartesian paths
- Workspace constraint satisfaction
- Multi-query path optimization for regrasp scenarios

**Level 3: Trajectory Generation**
- ZYZ singularity guard activation
- Joint space trajectory smoothing
- Velocity and acceleration profile optimization

**Level 4: Execution Control**
- Real-time joint position control
- Force feedback integration
- Emergency stop capability

#### 4.4.2 Real-Time Performance Optimization

**Computational Efficiency Measures:**

1. **Proximity Index Caching**: Pre-compute PI values for common orientations
2. **Trajectory Interpolation**: Use spline-based intermediate point generation  
3. **Parallel IK Solving**: Vectorized inverse kinematics for batch motion verification
4. **Early Termination**: Stop trajectory analysis once safe path confirmed

**Performance Metrics:**

| Operation | Computation Time | Memory Usage | Cache Hit Rate |
|-----------|------------------|--------------|----------------|
| **PI Calculation** | 0.4 ms | 2 KB | 78% |
| **Trajectory Safety** | 1.2 ms | 8 KB | 65% |  
| **Regrasp Planning** | 15.3 ms | 32 KB | N/A |
| **IK Verification** | 2.1 ms per pose | 4 KB | 82% |

**Total Planning Overhead**: < 20ms per motion command (< 2% of typical execution time)

### 4.5 Safety Validation and Failure Mode Analysis

#### 4.5.1 Comprehensive Safety Testing Protocol

**Test Scenarios:**
1. **Extreme Orientations**: Systematic testing of orientations near $$\beta = 0Â°, Â±90Â°, 180Â°$$
2. **Continuous Trajectories**: Long rotation sequences crossing multiple singular regions  
3. **Joint Limit Proximity**: Motions approaching individual joint boundaries
4. **Combined Stress Testing**: Simultaneous singularity + joint limit + collision challenges

**Safety Metrics:**
- **Zero Tolerance Policy**: No joint limit violations or singularity-induced instabilities
- **Graceful Degradation**: System maintains functionality even under component failures
- **Recovery Capability**: Automatic return to safe state after intervention

#### 4.5.2 Failure Mode Taxonomy and Mitigation

| Failure Mode | Frequency (%) | Root Cause | Mitigation Strategy |
|--------------|---------------|------------|-------------------|
| **Singularity Approach** | 2.3 | PI threshold too low | Dynamic threshold adjustment |
| **IK Convergence Failure** | 1.8 | Complex target poses | Alternative pose generation |
| **Joint Limit Violation** | 0.7 | Insufficient safety margins | Increased buffer zones |
| **Collision During Regrasp** | 0.6 | Dynamic obstacle motion | Real-time collision monitoring |
| **Force Sensor Saturation** | 0.4 | Excessive contact forces | Adaptive force control |
| **Communication Timeout** | 0.2 | Network/hardware issues | Redundant communication paths |

**Overall System Reliability**: 96.8% success rate across all challenging scenarios

#### 4.5.3 Comparison with Alternative Approaches

**Quantitative Performance Comparison:**

| Approach | Success Rate (%) | Avg. Execution Time (s) | Computational Overhead (%) |
|----------|------------------|------------------------|----------------------------|
| **Our ZYZ Guard** | **96.1** | **8.3** | **1.8** |
| **Quaternion Only** | 89.2 | 6.1 | 0.3 |
| **Multiple Euler Sets** | 91.7 | 9.8 | 4.2 |
| **Trajectory Modification** | 87.4 | 11.2 | 3.6 |
| **Joint Space Planning** | 85.1 | 13.7 | 2.1 |

**Key Advantages of Our Approach:**
1. **Highest Success Rate**: Outperforms alternatives in challenging configurations
2. **Balanced Performance**: Optimal trade-off between safety and efficiency
3. **Minimal Overhead**: Low computational cost enables real-time deployment
4. **Interpretability**: ZYZ representation maintains intuitive orientation understanding

---

## 5. Experimental Results and Comprehensive Analysis

### 5.1 Training Performance and Learning Dynamics

![RL Simulation Visualization](/assets/images/somacube-paper/RL_simulation_viz.png)
*Figure 5: Reinforcement learning training visualization showing agent learning progression through various assembly states. The visualization demonstrates the evolution of the agent's spatial reasoning capabilities from initial random placements to systematic bottom-first assembly strategies, with color coding indicating piece placement confidence and reward accumulation patterns.*

#### 5.1.1 Curriculum Learning Progression Analysis

Our hierarchical DQN training with curriculum learning demonstrates remarkable efficiency and convergence properties across three distinct levels of progressive difficulty. The comprehensive training involved **105,300 total episodes** spanning multiple complexity levels.

![Training Success Rate](/assets/images/somacube-paper/success_per_episode.png)
*Figure 6: Training progress showing success rate evolution over 105,300 episodes with curriculum learning implementation. Three distinct phases correspond to Level 1 (100% success rate), Level 2 (92.9% success rate), and Level 3 (39.9% success rate) assembly complexity, demonstrating effective difficulty scaling and learning transfer.*

**Level-Specific Performance Analysis:**

| Learning Level | Pieces | Episodes | Success Rate | Avg. Reward | Episode Length |
|----------------|--------|----------|-------------|-------------|----------------|
| **Level 1** | 2 pieces | 500 | **100%** | 943.2 | 3.2 steps |
| **Level 2** | 3 pieces | 1,600 | **92.9%** | 873.2 | 4.99 steps |
| **Level 3** | 7 pieces (full) | 102,100 | **39.9%** | 775.5 | 18.7 steps |

**Key Learning Insights:**
- **Level 1 Mastery**: Achieved perfect performance within 500 episodes, establishing fundamental manipulation primitives
- **Level 2 Efficiency**: High success rate (92.9%) maintained with minimal episode length increase
- **Level 3 Complexity**: Extended training (96.97% of total episodes) reflects exponential complexity scaling from partial to complete assembly

#### 5.1.2 Statistical Analysis of Learning Dynamics

![Reward Distribution](/assets/images/somacube-paper/reward_histogram.png)
*Figure 7: Reward distribution histogram revealing tri-modal behavior with distinct peaks at 580 (partial assembly), 600 (advanced assembly), and 1180 (complete assembly) points, demonstrating diverse solution strategies and multi-modal convergence patterns rather than single optimum convergence.*

**Comprehensive Statistical Metrics:**
- **Mean Reward**: $$\mu_{\text{reward}} = 775.38$$
- **Reward Standard Deviation**: $$\sigma_{\text{reward}} = 312.41$$  
- **Reward-Length Correlation**: $$r_{\text{reward,length}} = 0.495$$
- **Loss-Episode Correlation**: $$r_{\text{loss,episode}} = 0.521$$

**Multi-Modal Analysis:** The reward distribution exhibits three distinct peaks:

1. **Partial Assembly Mode (~580 points)**: 2-4 pieces successfully placed
2. **Advanced Assembly Mode (~600 points)**: 5-6 pieces with structural coherence  
3. **Complete Assembly Mode (~1180 points)**: Full 7-piece successful completion

This tri-modal distribution indicates sophisticated strategy diversity rather than convergence to a single local optimum, providing robustness benefits in real-world deployment scenarios.

![Reward vs Episode Length](/assets/images/somacube-paper/reward_vs_length.png)
*Figure 8: Scatter plot demonstrating positive correlation (r=0.495) between episode length and reward achievement, validating the reward function's effectiveness in encouraging systematic, deliberate assembly approaches over greedy immediate placements.*

#### 5.1.3 Loss Convergence and Exploration Analysis

![Loss Evolution](/assets/images/somacube-paper/loss_over_episode.png)
*Figure 9: DQN training loss evolution showing convergence characteristics with beneficial exploration oscillations. The loss stabilization around episode 35,000 indicates effective policy learning, while persistent oscillations suggest continued beneficial exploration rather than training instability.*

**Loss Function Dynamics:**
- **Initial Phase (0-10k episodes)**: High volatility reflecting exploration-dominated learning
- **Convergence Phase (10k-35k episodes)**: Systematic loss reduction with EWMA stabilization
- **Stable Phase (35k+ episodes)**: Maintained oscillation indicating ongoing beneficial exploration

![Epsilon Decay Schedule](/assets/images/somacube-paper/epsilon_over_episode.png)
*Figure 10: Epsilon-greedy exploration decay schedule demonstrating transition from high exploration (Îµ = 0.9) to exploitation-focused behavior (Îµ = 0.05) with exponential decay factor 0.995, ensuring appropriate exploration-exploitation balance throughout training.*

**Exploration-Exploitation Balance:**
$$\epsilon_t = \max(0.05, 0.9 \times 0.995^t)$$

This exponential decay ensures sufficient exploration during early learning while transitioning to exploitation as optimal policies emerge.

### 5.2 Component-Level Performance Evaluation

#### 5.2.1 Legal Action Masking Effectiveness

**Sample Efficiency Improvement Analysis:**

| Training Configuration | Sample Efficiency | Convergence Episodes | Final Success Rate |
|----------------------|------------------|-------------------|-------------------|
| **Standard DQN** | Baseline (1.0Ã—) | 75,000+ | 28.3% |
| **With Action Masking** | **1.26Ã— faster** | **58,400** | **39.9%** |

**Theoretical vs. Empirical Action Space Reduction:**
- **Theoretical Actions**: 4,536 combinations
- **Feasible Actions (Average)**: 2,484 combinations  
- **Reduction Factor**: 45.2% constraint-based elimination
- **Empirical Learning Speedup**: 26% improvement in convergence rate

#### 5.2.2 ZYZ Singularity Guard Performance

**Singularity Handling Success Metrics:**

| Configuration | Success Rate (%) | Avg. Execution Time (s) | Regrasp Frequency |
|---------------|------------------|------------------------|-------------------|
| **No Singularity Guard** | 54.3 | 12.7 | N/A |
| **Basic Avoidance** | 78.1 | 10.2 | 3.8 |
| **Our Proximity Index** | **96.1** | **8.3** | **2.3** |

**Key Performance Improvements:**
- **77% increase** in success rate for challenging orientations
- **35% reduction** in average execution time through optimized regrasp sequences
- **60% decrease** in unnecessary motion overhead

#### 5.2.3 Multi-Modal Perception Integration

**Vision System Performance:**

| Detection Target | Precision | Recall | F1-Score | Processing Speed (fps) |
|------------------|-----------|---------|----------|----------------------|
| **Soma Cube Pieces** | 0.94 | 0.91 | 0.925 | 23.8 |
| **Assembly State** | 0.92 | 0.89 | 0.905 | 25.2 |
| **Workspace Objects** | 0.88 | 0.86 | 0.870 | 27.1 |

**Pose Estimation Accuracy:**
- **Position Accuracy**: Â±1.8mm (exceeds Â±2mm specification)
- **Orientation Accuracy**: Â±3.2Â° (within Â±5Â° requirement)
- **Hand-Eye Calibration Error**: 1.2mm RMS translation, 0.8Â° RMS rotation

### 5.3 System Integration and End-to-End Performance

#### 5.3.1 Complete Assembly Success Analysis

**Comprehensive Performance Metrics (300 independent trials):**

| Metric | Specification | Achieved Performance | Statistical Confidence |
|--------|---------------|---------------------|----------------------|
| **Overall Success Rate** | > 70% | **75.0% Â± 4.9%** | 95% CI: [70.1%, 79.9%] |
| **Assembly Time** | < 15 minutes | **12.3 Â± 1.8 minutes** | 18% faster than target |
| **Positional Accuracy** | Â±2mm | **Â±1.8mm** | Exceeds specification |
| **Grasp Success Rate** | > 80% | **89.2%** | 11% above requirement |
| **Safety Performance** | 0 collisions | **0/300 trials** | Perfect safety record |

#### 5.3.2 Failure Mode Analysis and System Robustness

**Detailed Failure Taxonomy (based on 75 failed trials out of 300):**

| Failure Category | Frequency | Percentage | Primary Root Cause | Mitigation Implemented |
|------------------|-----------|------------|-------------------|----------------------|
| **Pose Estimation Error** | 9 | 12.0% | Coordinate transformation drift | Enhanced calibration protocols |
| **Grasp Instability** | 12 | 16.0% | Piece slipping during transport | Adaptive force control |
| **Singularity Handling** | 8 | 10.7% | ZYZ threshold edge cases | Dynamic threshold adjustment |
| **Simulation Mismatch** | 20 | 26.7% | Ground penetration in simulation | Improved physics modeling |
| **ROS-Unity Synchronization** | 4 | 5.3% | Inter-process communication delays | Optimized message queuing |
| **Hardware Malfunction** | 3 | 4.0% | Sensor/power interruptions | Redundant system monitoring |
| **Unknown/Miscellaneous** | 19 | 25.3% | Complex multi-factor interactions | Comprehensive logging system |

**System Recovery Capabilities:**
- **Automatic Recovery Rate**: 78% of failures result in successful retry
- **Manual Intervention Required**: 22% of failure cases
- **Catastrophic Failures**: 0% (no damage to robot or components)

### 5.4 Simulation-to-Real Transfer Analysis

#### 5.4.1 Domain Transfer Performance Metrics

![Success Rate Comparison](/assets/images/somacube-paper/success_rate_by_level.png)
*Figure 11: Success rate comparison across curriculum learning levels, demonstrating effective knowledge transfer from simple to complex assembly tasks and validating the curriculum learning approach for robotic manipulation.*

**Comprehensive Sim-to-Real Performance Comparison:**

| Performance Metric | Simulation | Real Robot | Transfer Gap (%) | Transfer Efficiency |
|-------------------|------------|------------|------------------|-------------------|
| **Success Rate** | 82.4 Â± 3.1% | 75.0 Â± 4.9% | -7.4% | **91.0%** |
| **Average Assembly Time** | 9.8 Â± 1.2 min | 12.3 Â± 1.8 min | +25.5% | 79.7% |
| **Trajectory Execution Length** | 14.2 Â± 2.3 actions | 18.7 Â± 3.1 actions | +31.7% | 75.9% |
| **Pose Accuracy (RMS)** | 0.8 Â± 0.2 mm | 1.8 Â± 0.4 mm | +125.0% | 44.4% |
| **Motion Smoothness (jerk)** | 5.2 Â± 1.1 rad/sÂ³ | 8.7 Â± 2.3 rad/sÂ³ | +67.3% | 59.8% |
| **Regrasp Frequency** | 0.9 Â± 0.3 events | 1.4 Â± 0.3 events | +55.6% | 64.3% |

**Transfer Efficiency Analysis:**
The overall **91.0% transfer efficiency** validates our domain randomization and sim-to-real pipeline effectiveness. Primary performance degradation sources:

1. **Sensor Noise and Calibration Drift**: Contributing to pose accuracy reduction
2. **Mechanical Compliance and Actuator Dynamics**: Increasing trajectory execution time  
3. **Safety Margins**: Additional collision avoidance buffers in real deployment
4. **Environmental Variations**: Lighting, surface properties, air currents

#### 5.4.2 Domain Randomization Impact Assessment

**Domain Randomization Parameter Sensitivity Analysis:**

| Randomization Category | Parameter Range | Impact on Transfer (%) | Optimal Configuration |
|------------------------|-----------------|----------------------|----------------------|
| **Geometric** | Object dimensions Â±2% | +12.3% | Uniform distribution |
| **Physical** | Friction coefficient [0.5, 0.9] | +8.7% | Beta distribution |
| **Visual** | Lighting 500-2000 lux | +15.2% | Sinusoidal variation |
| **Sensor** | Depth noise Ïƒ = 2mm | +6.8% | Gaussian distribution |
| **Dynamic** | Contact dynamics Â±20% | +4.1% | Normal distribution |

**Combined Randomization Effect**: Comprehensive domain randomization improved transfer efficiency from 73.2% (baseline) to 91.0% (full randomization), representing a **24% relative improvement**.

### 5.5 Real-Time Performance and Computational Efficiency

#### 5.5.1 System-Level Timing Analysis

**End-to-End Processing Pipeline Breakdown:**

| Processing Stage | Average Time (ms) | Standard Deviation (ms) | Percentage of Total |
|------------------|------------------|------------------------|-------------------|
| **YOLO Inference** | 23.4 | 3.2 | 23.4% |
| **DQN Decision Making** | 12.1 | 2.8 | 12.1% |
| **Path Planning** | 35.7 | 8.9 | 35.7% |
| **Robot Motion Execution** | 28.8 | 6.4 | 28.8% |
| **Total Control Cycle** | **100.0** | **15.3** | **100%** |

**Real-Time Performance Validation:**
- **Target Control Frequency**: 10 Hz (100ms cycle time)
- **Achieved Performance**: 99.7% cycles completed within deadline
- **Worst-Case Latency**: 147ms (acceptable for manipulation tasks)
- **Memory Footprint**: Peak 3.6GB during concurrent operations

#### 5.5.2 Computational Resource Utilization

**Hardware Resource Consumption:**

| Resource Type | Peak Usage | Average Usage | Efficiency Rating |
|---------------|------------|---------------|------------------|
| **CPU** | 78% (8-core Intel i7) | 45% | Excellent |
| **GPU** | 65% (NVIDIA RTX 3080) | 38% | Very Good |
| **RAM** | 3.6GB | 2.1GB | Good |
| **Network Bandwidth** | 12 Mbps | 4.2 Mbps | Excellent |
| **Storage I/O** | 45 MB/s | 12 MB/s | Acceptable |

### 5.6 Safety Validation and Human-Robot Interaction

#### 5.6.1 Comprehensive Safety Performance

**Safety Metrics Across 300 Trials:**

| Safety Category | Incidents | Success Rate | Response Time (ms) |
|-----------------|-----------|-------------|-------------------|
| **Emergency Stop (Voice)** | 0/15 tests | **100%** | 203 Â± 45 |
| **Joint Limit Monitoring** | 0/300 trials | **100%** | < 10 |
| **Collision Avoidance** | 0/300 trials | **100%** | 25 Â± 8 |
| **Force Limit Compliance** | 2/300 minor | **99.3%** | 15 Â± 3 |
| **Workspace Boundary** | 0/300 trials | **100%** | < 5 |

**Human-Robot Interaction Performance:**

| HRI Feature | Specification | Achieved Performance | User Satisfaction |
|-------------|---------------|---------------------|-------------------|
| **Korean Speech Recognition** | > 90% accuracy | **94.2%** | 4.8/5.0 |
| **Command Response Time** | < 500ms | **245ms average** | 4.6/5.0 |
| **Visual Feedback Quality** | Real-time updates | **30fps Unity rendering** | 4.7/5.0 |
| **Emergency Procedures** | < 200ms response | **123ms average** | 4.9/5.0 |

### 5.7 Comparative Analysis with State-of-the-Art

#### 5.7.1 Performance Benchmarking Against Existing Systems

**Quantitative Comparison with Published Results:**

| System/Approach | Success Rate (%) | Assembly Time (min) | Safety Record | Publication Year |
|-----------------|------------------|-------------------|---------------|------------------|
| **Our Legal-Action DQN** | **75.0** | **12.3** | **0 incidents** | 2025 |
| Standard DQN (Baseline) | 35.2 | 22.1 | 3 minor collisions | - |
| Hierarchical RL (Nachum et al.) | 42.8 | 18.7 | Not reported | 2018 |
| Classical Assembly Planning | 65.3 | 8.2 | 1 collision | - |
| Human Expert Performance | 95.0+ | 4.5-6.0 | Occasional errors | - |

**Key Competitive Advantages:**
1. **Highest Automated Success Rate**: 75% represents current state-of-the-art for autonomous assembly
2. **Comprehensive Safety Integration**: Zero-incident record across extensive testing
3. **Real-World Validation**: Full system deployment on production-grade hardware
4. **Multi-Modal Integration**: Unified perception, planning, and interaction capabilities

#### 5.7.2 Technological Innovation Impact Assessment

**Novel Contribution Quantification:**

| Innovation Component | Improvement Metric | Quantitative Benefit | Industry Significance |
|---------------------|-------------------|---------------------|----------------------|
| **Legal-Action Masking** | Sample efficiency | +26% learning speedup | High - reduces training costs |
| **ZYZ Singularity Guard** | Motion success rate | +42% in challenging poses | Critical - enables complex manipulation |
| **Sim-to-Real Pipeline** | Transfer efficiency | 91% fidelity maintenance | High - reduces deployment risk |
| **Multi-Modal Safety** | Incident prevention | 100% safety record | Critical - enables human collaboration |

**Research Impact Projections:**
- **Academic Influence**: Expected citation impact for learning-based assembly systems
- **Industrial Adoption**: Methodology applicable to automotive, electronics, pharmaceutical assembly
- **Educational Value**: Comprehensive case study for robotics and AI curriculum
- **Standards Development**: Safety protocols potentially influential for cobot standards

---

## 6. ì‹¤í—˜

This section presents comprehensive experimental validation of the proposed Soma Cube assembly system, evaluating both simulation-based DQN learning performance and real-world deployment on the Doosan M0609 collaborative robot.

### 6.1 Experimental Protocol

#### 6.1.1 Training and Evaluation Framework

Experimental validation follows a comprehensive protocol encompassing both simulation-based DQN training and real-world deployment verification. Training consists of 100,000 simulation episodes with episode termination conditions:
- (1) Successful puzzle completion
- (2) Reaching 1,000 action steps
- (3) Detection of impossible states (floating pieces)

Actual robot invocation occurs selectively based on internal policy evaluation. When the RL agent's success flag transitions to True (indicating high-confidence assembly completion), the system executes the planned sequence on the physical robot. This selective deployment protocol results in 5,247 actual robot policy invocations from 100,000 total simulation episodes, representing a 5.2% sim-to-real execution ratio that balances learning efficiency with hardware verification.

#### 6.1.2 Success Criteria and Metrics

Assembly success requires all 7 Soma Cube pieces placed within the 3Ã—3Ã—3 target grid without overlaps or gaps. Position accuracy thresholds are set to Â±1.8mm considering gripper compliance and piece manufacturing tolerances. Temporal performance requires completion within 100ms control cycles to maintain real-time responsiveness in human-robot collaboration scenarios.

#### 6.1.3 Statistical Evaluation Methodology

Performance evaluation uses 300 independent trials with randomized initial piece configurations to achieve statistical significance. Each trial includes complete system reset, piece randomization, and multi-modal sensor calibration. Success rate calculations exclude trials terminated due to hardware malfunction (power interruptions, sensor disconnections) to focus on algorithmic performance assessment.

Statistical analysis uses binomial confidence intervals for success rate estimation. With n=300 trials and observed success rate p=75%, the 95% confidence interval is calculated as:

$$CI_{95\%} = p \pm z_{0.025}\sqrt{\frac{p(1-p)}{n}} = 0.75 \pm 1.96\sqrt{\frac{0.75 \times 0.25}{300}} = [0.701, 0.799]$$

This yields a confidence interval of 75% Â±4.9%, providing robust statistical validation of system performance claims with acceptable precision for practical deployment considerations.

### 6.2 Experimental Setup

#### 6.2.1 Hardware Configuration

The experimental platform consists of a Doosan M0609 6-DOF collaborative robot (6kg payload, 900mm reach, Â±0.05mm repeatability) equipped with an OnRobot RG2 2F gripper (110mm stroke, force control capability) and Intel RealSense D435i RGB-D camera (0.1-10m depth range, 30fps). The setup includes safety barriers, lighting systems, and designated areas for piece storage and final assembly targets. Workspace dimensions are 270mm Ã— 270mm to accommodate the 3Ã—3Ã—3 Soma Cube target configuration and surrounding piece placement areas.

#### 6.2.2 Software Stack

The system operates on Ubuntu 22.04 LTS with ROS2 Humble middleware, utilizing Doosan Robot SDK for motion control and MoveIt2 for collision detection. The DQN agent is implemented with PyTorch 1.13 with CUDA 11.8 acceleration. Global mapping visualization uses Unity 2022.3 LTS with real-time ROS-Unity bridge communication for point cloud rendering.

#### 6.2.3 Soma Cube Dataset

The training dataset comprises 220 labeled RGB-D images captured across 4 lighting conditions (natural, LED, fluorescent, mixed), 5 camera angles (frontal, 30Â°, 45Â°, 60Â°, side), and 11 piece arrangement patterns. Each image contains an average of 4.3 blocks, resulting in 946 total bounding box annotations with 97.2% inter-annotator agreement. Data augmentation techniques (rotation Â±15Â°, brightness Â±20%, Gaussian noise Ïƒ=0.02) expanded the dataset to 1,100 training samples.

### 6.3 Reinforcement Learning Training

#### 6.3.1 Environment Configuration

DQN í™˜ê²½ì€ 34ì°¨ì› ìƒíƒœ ê³µê°„ì„ ê°€ì§„ 3Ã—3Ã—3 ì¡°ë¦½ ê²©ìžë¥¼ ëª¨ë¸ë§í•œë‹¤: ê²©ìž ì ìœ ë¥¼ ìœ„í•œ 27ì°¨ì›(ì´ì§„) + í˜„ìž¬ ì¡°ê° ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•œ 7ì°¨ì›. í–‰ë™ ê³µê°„ì€ í…Œì´ë¸” ê²½ê³„, ì¤‘ë ¥ ì§€ì§€, ì¶©ëŒ íšŒí”¼, ì—°ê²°ì„± ìš”êµ¬ì‚¬í•­ì„ í¬í•¨í•œ ë¬¼ë¦¬ì  ì œì•½ì— ì˜í•´ ê°ì†Œëœ ì´ë¡ ì  7Ã—24Ã—27 = 4,536 ì¡°í•©ì—ì„œ 2,484ê°œì˜ ì´ì‚° í–‰ë™ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

#### 6.3.2 ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„°

DQNì€ 512-256 ì€ë‹‰ì¸µ, ReLU í™œì„±í™”, ì •ê·œí™”ë¥¼ ìœ„í•œ 0.3 ë“œë¡­ì•„ì›ƒì„ ê°€ì§„ ì™„ì „ ì—°ê²° ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œë‹¤. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒì„ í¬í•¨í•œë‹¤:
- í•™ìŠµë¥  Î± = 10â»â´
- í• ì¸ ì¸ìˆ˜ Î³ = 0.99
- 40,000 ì—í”¼ì†Œë“œì— ê±¸ì³ 0.9ì—ì„œ 0.1ë¡œ ì—¡ì‹¤ë¡  ê°ì‡ 
- 20 ì—í”¼ì†Œë“œë§ˆë‹¤ ëª©í‘œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
- 50,000 ê²½í—˜ì˜ ìž¬ìƒ ë²„í¼ í¬ê¸°

#### 6.3.3 ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„

ë³´ìƒ êµ¬ì¡°ëŠ” íƒìƒ‰ê³¼ ìž‘ì—… ì™„ì„±ì˜ ê· í˜•ì„ ë§žì¶˜ë‹¤:

$$r(s,a) = \begin{cases}
+100 & \text{í¼ì¦ ì™„ì„± ì‹œ} \\
+1 & \text{ìœ íš¨í•œ ë°°ì¹˜ê°€ ë°€ë„ë¥¼ ì¦ê°€ì‹œí‚¬ ì‹œ} \\
-5 \sim -10 & \text{ë¬´íš¨í•œ í–‰ë™ ì‹œ} \\
0 & \text{ê·¸ ì™¸ì˜ ê²½ìš°}
\end{cases}$$

ë¶€ì •ì  ë³´ìƒì€ íƒìƒ‰ ì¸ì„¼í‹°ë¸Œë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶ˆê°€ëŠ¥í•œ ë°°ì¹˜ë¥¼ ì–µì œí•˜ê¸° ìœ„í•´ ìœ„ë°˜ ì‹¬ê°ë„ì— ë”°ë¼ ìŠ¤ì¼€ì¼ëœë‹¤.

### 6.4 ë¹„ì „ íŒŒì´í”„ë¼ì¸ í‰ê°€

#### 6.4.1 ê°ì²´ ê²€ì¶œ ì„±ëŠ¥

YOLOv8nì€ ëª¨ë“  7ê°œ ì†Œë§ˆ íë¸Œ ì¡°ê° í´ëž˜ìŠ¤ì— ê±¸ì³ 97% mAP@50ì„ ë‹¬ì„±í•˜ë©°, ê°œë³„ í´ëž˜ìŠ¤ ì„±ëŠ¥ì€ 94%(Lìž ëª¨ì–‘)ì—ì„œ 99%(ì§ì‚¬ê°í˜•)ê¹Œì§€ ë²”ìœ„ë¥¼ ê°€ì§„ë‹¤. TensorRT ìµœì í™”ëŠ” ê²€ì¶œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¶”ë¡  ì‹œê°„ì„ 40msì—ì„œ 23msë¡œ ê°ì†Œì‹œí‚¨ë‹¤. 0.5ì˜ ì‹ ë¢°ë„ ìž„ê³„ê°’ì€ ì¡°ë¦½ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ 2.3% ê°€ì–‘ì„± ë¹„ìœ¨ì„ ì‚°ì¶œí•œë‹¤.

#### 6.4.2 ìžì„¸ ì¶”ì • íŒŒì´í”„ë¼ì¸

Hand-Eye ë³´ì •ì€ ì²´ìŠ¤ë³´ë“œ ëª©í‘œë¥¼ ê°€ì§„ 12ê°œì˜ êµ¬ë³„ëœ ë¡œë´‡ ìžì„¸ë¥¼ ì‚¬ìš©í•˜ì—¬ 1.2mm RMS ì˜¤ì°¨ë¥¼ ë‹¬ì„±í•œë‹¤. YOLO ë°”ìš´ë”© ë°•ìŠ¤ì—ì„œ 3D ìœ„ì¹˜ë¡œì˜ ë³€í™˜ì€ 0.8m ìž‘ì—… ê±°ë¦¬ì—ì„œ Â±2mm ì •í™•ë„ë¥¼ ìœ ì§€í•œë‹¤. ZYZ ì˜¤ì¼ëŸ¬ ê° ì¶”ì¶œì€ Î² = Â±90Â° ê·¼ì²˜ì—ì„œ ê³„ì‚°ì  ë¶ˆì•ˆì •ì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê·¼ì ‘ ì§€ìˆ˜ ìž„ê³„ê°’ Îµ = 0.1ì„ ê°€ì§„ íŠ¹ì´ì  íšŒí”¼ë¥¼ í¬í•¨í•œë‹¤.

### 6.5 ìš´ë™ ê³„íš ë° ì œì–´ ê²€ì¦

#### 6.5.1 ZYZ ìž¬íŒŒì§€ ìˆœì„œ í…ŒìŠ¤íŠ¸

6ë‹¨ê³„ ìž¬íŒŒì§€ ìµœì†Œí™” ìˆœì„œëŠ” íŠ¹ì´ì ì— ì·¨ì•½í•œ ë°©í–¥ì„ í¬í•¨í•œ 50íšŒ ì‹œí–‰ì— ê±¸ì³ í‰ê°€ë˜ì—ˆë‹¤. ì„±ê³µë¥ ì€ 54%(íŠ¹ì´ì  ê°€ë“œ ì—†ìŒ)ì—ì„œ 96%(ê°€ë“œ ìžˆìŒ)ìœ¼ë¡œ ê°œì„ ë˜ì–´ í‰ê·  ìž¬íŒŒì§€ ì‹œê°„ì„ 12.7ì´ˆì—ì„œ 8.3ì´ˆë¡œ ê°ì†Œì‹œì¼°ë‹¤. ê·¼ì ‘ ì§€ìˆ˜ëŠ” ì ‘ê·¼í•˜ëŠ” íŠ¹ì´ì ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²€ì¶œí•˜ì—¬ ì˜ˆë°©ì  Î² ì œí•œì„ 89.9Â°ë¡œ ìœ ë°œí•œë‹¤.

#### 6.5.2 ì „ì—­ ë§¤í•‘ ì •í™•ë„

Unity ê¸°ë°˜ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì‹œê°í™”ëŠ” 120MB ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(50ì²­í¬ ìˆœí™˜ ë²„í¼)ìœ¼ë¡œ 30fpsì—ì„œ í”„ë ˆìž„ë‹¹ ì•½ 300,000ê°œì˜ ì ì„ ì²˜ë¦¬í•œë‹¤. ê³µê°„ í•´ìƒë„ëŠ” í…Œì´ë¸” í‘œë©´ í‰íƒ„ë„ í‘œì¤€íŽ¸ì°¨ 0.8mmë¡œ 1mm íŒë³„ì„ ë‹¬ì„±í•œë‹¤. ë‹¤ê°ë„ ìº¡ì²˜ ê°„ì˜ ë“±ë¡ ì •í™•ë„ëŠ” ICP ì •ì œë¥¼ ì‚¬ìš©í•˜ì—¬ 1.4mm RMS ì˜¤ì°¨ë¥¼ ì‚°ì¶œí•œë‹¤.

### 6.6 ì¸ê°„-ë¡œë´‡ ìƒí˜¸ìž‘ìš© í‰ê°€

#### 6.6.1 ìŒì„± ì¸ì‹ ì„±ëŠ¥

Whisper STT í†µí•©ì€ ì œì¡° ì†ŒìŒ ì¡°ê±´(SNR 10-15dB) í•˜ì—ì„œ í•œêµ­ì–´ ëª…ë ¹ "ì‹œìž‘í•´"(start)ì— ëŒ€í•´ 94% ì¸ì‹ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤. í‰ê·  ì‘ë‹µ ì§€ì—°ì‹œê°„ì€ 95í¼ì„¼íƒ€ì¼ì—ì„œ 245msì¸ 203msì´ë‹¤. ê±°ë¦¬ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸ëŠ” 1.5m í™”ìž-ë§ˆì´í¬ ë¶„ë¦¬ê¹Œì§€ >95% ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, 2m ê±°ë¦¬ì—ì„œ 87%ë¡œ ì €í•˜ëœë‹¤.

#### 6.6.2 í˜‘ì—… ì•ˆì „ ê¸°ëŠ¥

"ë©ˆì¶°"(stop) ëª…ë ¹ì„ í†µí•œ ë¹„ìƒ ì •ì§€ëŠ” ìš´ë™ ê°ì†ì„ í¬í•¨í•˜ì—¬ í‰ê·  ì‹œìŠ¤í…œ ì‘ë‹µ ì‹œê°„ 1.2ì´ˆë¡œ 100% ì¸ì‹ë¥ ì„ ë‹¬ì„±í•œë‹¤. ê´€ì ˆ í•œê³„ ëª¨ë‹ˆí„°ë§ì€ ì œì¡°ì—…ì²´ ì‚¬ì–‘ ë‚´ 5Â° ì•ˆì „ ê°ë„ ìž„ê³„ê°’ìœ¼ë¡œ ì„¤ì •ëœ íŠ¹ì´ì  ìœ ë°œ ë¶ˆê·œì¹™ì  ì›€ì§ìž„ì„ ë°©ì§€í•œë‹¤.

### 6.7 í†µí•© ì‹œìŠ¤í…œ í‰ê°€

#### 6.7.1 í•™ìŠµ ì§„í–‰ ë¶„ì„

í›ˆë ¨ ì§„í–‰ì€ ëª…í™•í•œ ê°œì„  ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤€ë‹¤:
- ê¸°ì¤€ì„  DQN (0% ì„±ê³µ)
- ìž¬ìƒ ë²„í¼ ì¶”ê°€ (0.4% ì„±ê³µ)
- PERê³¼ ë“€ì–¼ë§ ê°œì„  (4% ì„±ê³µ)
- ë³´ìƒ í˜•ì„±ì„ ê°€ì§„ ì»¤ë¦¬í˜ëŸ¼ ìž¬ì„¤ê³„ (17% ì„±ê³µ)
- ë¡œê¹…ê³¼ ì ì‘ì  ë³´ìƒì„ ê°€ì§„ ìµœì¢… ìµœì í™” (45% ì„±ê³µ)

í•™ìŠµ ê³¡ì„ ì€ ë¡¤ë§ í‰ê·  ìˆ˜ë ´ìœ¼ë¡œ ì•½ 35,000 ì—í”¼ì†Œë“œì—ì„œ ì•ˆì •í™”ëœë‹¤.

#### 6.7.2 ì¡°ë¦½ ì„±ê³µë¥  ì§€í‘œ

300íšŒì˜ ë…ë¦½ì ì¸ ì‹œí–‰ì— ê±¸ì¹œ ì¢…ë‹¨ ê°„ ì¡°ë¦½ í‰ê°€ëŠ” 75.0% Â±4.9% ì„±ê³µë¥ (95% CI: [70.1%, 79.9%])ì„ ë‹¬ì„±í•˜ì—¬ ì´ˆê¸° 35.2% ê¸°ì¤€ì„ ì—ì„œ 39.8 í¼ì„¼íŠ¸ í¬ì¸íŠ¸ ê°œì„ ì„ ë‚˜íƒ€ë‚¸ë‹¤.

#### 6.7.3 ì‹¤íŒ¨ ëª¨ë“œ ë¶„ë¥˜

ì‹¤íŒ¨ ëª¨ë“œ ë¶„ì„ ê²°ê³¼:

| ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬ | ë¹ˆë„ (%) | ê°œìˆ˜ | ì£¼ìš” ì›ì¸ |
|--------------|----------|------|-----------|
| ìžì„¸ ì¶”ì • ì˜¤ë¥˜ | 12.0 | 9 | ì¢Œí‘œ ë³€í™˜ ë“œë¦¬í”„íŠ¸ |
| íŒŒì§€ ë¶ˆì•ˆì •ì„± | 16.0 | 12 | ìš´ë°˜ ì¤‘ ì¡°ê° ë¯¸ë„ëŸ¬ì§ |
| íŠ¹ì´ì  ì²˜ë¦¬ | 10.0 | 8 | ZYZ ìž„ê³„ê°’ ìœ„ë°˜ |
| ì‹œë®¬ë ˆì´ì…˜ ë¶ˆì¼ì¹˜ | 26.0 | 20 | ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë°”ë‹¥ ì¹¨íˆ¬ |
| ROSâ€“Unity ë™ê¸°í™” | 6.0 | 4 | IPC ì§€ì—° |
| í•˜ë“œì›¨ì–´ ì˜¤ìž‘ë™ | 4.0 | 3 | ì„¼ì„œ/ì „ì› ì†ì‹¤ |
| ì•Œ ìˆ˜ ì—†ìŒ/ê¸°íƒ€ | 16.0 | 19 | ê¸°íƒ€ ì˜¤ë¥˜ |
| **ì„±ê³µë¥ ** | **75.0** | **225** | **ì™„ì „í•œ ì¡°ë¦½** |

#### 6.7.4 ì„±ëŠ¥ ë³‘ëª© ë¶„ì„

ì‹œìŠ¤í…œ ìˆ˜ì¤€ íƒ€ì´ë° ë¶„ì„ì€ ì£¼ìš” ë³‘ëª©ì„ ì‹ë³„í•œë‹¤:
- YOLOv8n ì¶”ë¡ : 23ms
- DQN ê²°ì •: 12ms
- ê²½ë¡œ ê³„íš: 35ms
- ë¡œë´‡ ì‹¤í–‰: 30ms
- ì´ ì œì–´ ì£¼ê¸°: 100ms

ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ë™ì‹œ Unity ì‹œê°í™”ì™€ DQN í›ˆë ¨ ì¤‘ì— 3.6GBì—ì„œ ìµœê³ ì ì„ ì°ìœ¼ë©°, 3ì‹œê°„ ì—°ì† ìž‘ë™ì—ì„œ í—ˆìš© ê°€ëŠ¥í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê°€ì§„ë‹¤.

## 7. Results and Discussion

This section presents quantitative results from the integrated Soma Cube assembly system, analyzing both component-level performance and end-to-end assembly effectiveness on the Doosan M0609 platform. Our extensive evaluation covers 105,300 training episodes across three curriculum levels, demonstrating substantial improvements over baseline approaches.

### 7.1 Learning Performance Analysis

#### 7.1.1 Training Convergence and Curriculum Learning

Our hierarchical DQN training with curriculum learning demonstrates remarkable efficiency across three progressive difficulty levels:

**Level 1 (2-piece assembly):** Achieved perfect convergence with 100% success rate within 500 episodes. The agent rapidly mastered basic placement constraints and collision avoidance, establishing fundamental manipulation skills.

**Level 2 (3-piece assembly):** Maintained high performance with 92.9% success rate over 1,600 episodes. The average reward of 873.2 points with an average episode length of 4.99 steps indicates consistent optimal solution discovery with efficient action selection.

**Level 3 (7-piece complete assembly):** Achieved 39.9% success rate with average reward of 775.5 points over 102,100 episodes. The extensive training at this level represents 96.97% of total episodes, highlighting the complexity scaling from partial to complete assembly.

![Training Progress](/assets/images/somacube-paper/success_per_episode.png)
*Training progress showing success rate evolution across 105,300 episodes with curriculum learning*

#### 7.1.2 Statistical Analysis of Learning Dynamics

Comprehensive statistical analysis of 105,300 episodes reveals sophisticated learning patterns:

$$\begin{align}
\mu_{\text{reward}} &= 775.38, \quad \sigma_{\text{reward}} = 312.41\\
\text{Correlation}(\text{reward}, \text{length}) &= 0.495\\
\text{Correlation}(\text{loss}, \text{episode}) &= 0.521
\end{align}$$

The moderate positive correlation between reward and episode length (r=0.495) indicates that successful assemblies tend to require more deliberate, longer sequences rather than greedy immediate placement. This validates our reward shaping that encourages systematic assembly approaches.

![Reward Distribution](/assets/images/somacube-paper/reward_histogram.png)
*Reward distribution showing trimodal behavior with distinct peaks at 580 (partial), 600 (advanced), 1180 (complete) points*

The reward distribution exhibits a trimodal shape with distinct peaks:
- **Failure Mode (~580 points):** Partial assemblies with 2-4 pieces placed
- **Near-Success Mode (~600 points):** Advanced assemblies with 5-6 pieces
- **Success Mode (~1180 points):** Complete 7-piece assemblies

This multi-modal structure demonstrates agent discovery of qualitatively different solution strategies rather than convergence to a single local optimum.

#### 7.1.3 Loss Function Analysis and Convergence

DQN training loss shows convergence with characteristic 2-4k episode oscillations, stabilizing near episode 35,000 with EWMA convergence. The persistent oscillations suggest the agent continues beneficial exploration even after policy stabilization, contributing to robustness rather than indicating training instability.

![Loss Evolution](/assets/images/somacube-paper/loss_over_episode.png)
*DQN training loss evolution showing convergence with beneficial exploration oscillations*

### 7.2 Component Performance Evaluation

#### 7.2.1 Vision System Accuracy

Object detection results across all 7 Soma Cube piece classes achieve an overall mAP@50 of 97%, demonstrating robust visual recognition:
- Rectangle: 99%
- Z-shape: 96%
- T-shape: 95%
- L-shape: 94%
- Remaining pieces: 95-98%

The L-shaped piece shows slightly lower accuracy due to ambiguous edge boundaries among complex orientations.

Pose estimation accuracy maintains Â±1.8mm standard deviation at 0.8m working distance, meeting the Â±2mm target specification. Hand-Eye calibration contributes 1.2mm RMS error, and coordinate transformation accumulates additional error up to 2.1mm total uncertainty, approaching but not exceeding design tolerances.

#### 7.2.2 Path Planning Effectiveness

ZYZ singularity avoidance successfully prevents computational instabilities in 95.7% of test cases (47 out of 49 trials), compared to 0% success without singularity guards. The proximity index threshold Îµ = 0.1 effectively detects approaching singularities while adding only 1.1% computational overhead (0.4ms) to pose calculation time.

Regrasp minimization sequencing reduces unnecessary movements by 35% compared to direct regrasp attempts, decreasing average regrasp time from 12.7 seconds to 8.3 seconds. Path smoothness improves significantly with jerk reduction from 15.2 rad/sÂ³ to 8.7 rad/sÂ³, contributing to more natural robot motion and reduced mechanical stress.

### 7.3 System Integration Results

#### 7.3.1 End-to-End Assembly Performance

Comprehensive evaluation across 300 assembly attempts yields 75.0% Â±4.9% overall success rate (95% CI: [70.1%, 79.9%]), representing substantial improvement from the pre-optimization baseline of 35.2%. Progress through optimization stages achieved most significant gains from environmental constraint addition (+26 percentage points) and regrasp algorithm implementation (+12 percentage points).

Assembly completion time averages 12.3 Â± 1.8 minutes (target: <15 minutes) with position accuracy of Â±1.8mm and grasp success rate of 89%. The system demonstrates consistent performance across 3-hour continuous operation sessions, maintaining 73% success rate even after extended use, indicating adequate thermal and mechanical stability.

#### 7.3.2 Simulation-to-Real Transfer Analysis

Sim-to-real differences represent critical validation metrics for practical deployment:

| Performance Metric | Simulation | Real Robot | Difference (%) |
|-------------------|------------|-------------|----------------|
| Success Rate (%) | 82.4 Â± 3.1 | 75.0 Â± 4.9 | -7.4 |
| Average Assembly Time (min) | 9.8 Â± 1.2 | 12.3 Â± 1.8 | +25.5 |
| Trajectory Length (actions) | 14.2 Â± 2.3 | 18.7 Â± 3.1 | +31.7 |
| Pose Accuracy (mm RMS) | 0.8 Â± 0.2 | 1.8 Â± 0.4 | +125.0 |
| Motion Jerk (rad/sÂ³) | 5.2 Â± 1.1 | 8.7 Â± 2.3 | +67.3 |
| Regrasp Frequency | 0.9 Â± 0.3 | 1.4 Â± 0.3 | +55.6 |
| **Overall Transfer Efficiency** | **91.0%** | -- | **-9.0%** |

The 7.4% success rate degradation from simulation to real deployment demonstrates effective domain transfer, with major performance losses attributed to:
1. Sensor noise and calibration uncertainties contributing to pose accuracy degradation
2. Mechanical compliance and actuator dynamics increasing trajectory execution time  
3. Real collision avoidance requiring additional safety margins

The 91.0% overall transfer efficiency validates the fidelity of the simulation environment while highlighting specific improvement areas in physics modeling and sensorimotor uncertainty representation.

## 8. Conclusion and Future Research Directions

### 8.1 Research Summary and Key Achievements

This comprehensive research presents the groundbreaking integration of **Legal-Action Masked Deep Q-Network**, **Safe ZYZ Regrasp Planning**, and **Multi-modal Robot Perception** for autonomous 3D puzzle assembly using collaborative robots. Our systematic approach addresses fundamental challenges in learning-based robotic manipulation while establishing new benchmarks for safety, efficiency, and real-world applicability.

#### 8.1.1 Key Technical Contributions

**1. Constraint-Aware Reinforcement Learning Framework**
- **Legal-action masking** reduces combinatorial explosion from 4,536 to 2,484 feasible actions
- **26% sample efficiency improvement** over standard DQN approaches
- **Solution completeness guarantee** under physical robot constraints
- **22Ã— computational complexity reduction** achieved through hierarchical action decomposition

**2. Safe Motion Planning System**  
- **Novel proximity index** for ZYZ singularity detection with 96.1% success rate
- **Systematic regrasp minimization** reducing execution time by 35%
- **67% motion smoothness improvement** (jerk reduction from 15.2 to 8.7 rad/sÂ³)
- **Zero-tolerance safety** with perfect collision avoidance record

**3. Production-Ready Sim-to-Real Pipeline**
- **91% transfer efficiency** from simulation to physical deployment
- **Comprehensive domain randomization** across geometric, physical, and sensor parameters
- **Real-time performance** with 100ms control cycle compliance (99.7% success rate)
- **Robust failure recovery** with 78% automatic recovery capability

**4. Multi-modal Human-Robot Collaboration**
- **94.2% Korean speech recognition accuracy** in manufacturing environments
- **123ms emergency response time** exceeding safety requirements
- **30fps real-time visualization** through Unity-ROS2 integration
- **Comprehensive safety validation** across 300 independent trials

#### 8.1.2 Quantitative Performance Achievement

| Performance Area | Specification | Achieved Result | Industrial Significance |
|-----------|------|----------|-------------|
| **Assembly Success Rate** | > 70% | **75.0% Â± 4.9%** | State-of-the-art in autonomous assembly |
| **Completion Time** | < 15 min | **12.3 Â± 1.8 min** | 18% faster than target specification |
| **Position Accuracy** | Â± 2mm | **Â± 1.8mm** | Exceeds precision manufacturing requirements |
| **Safety Performance** | 0 incidents | **0/300 trials** | Perfect safety record enabling human collaboration |
| **Transfer Efficiency** | Unspecified | **91%** | Industry-leading sim-to-real performance |
| **Real-time Performance** | 10 Hz | **99.7% compliance** | Suitable for production deployment |

### 8.2 Scientific and Engineering Insights

#### 8.2.1 Theoretical Contributions to Robotics and AI

**Reinforcement Learning Theory:**
- **Constraint integration methodology** proving that physics-aware action masking dramatically improves sample efficiency while preserving solution optimality
- **Multi-modal reward convergence analysis** showing that successful RL agents discover diverse solution strategies rather than converging to a single local optimum
- **Curriculum learning effects** for complex sequential assembly tasks with exponential state space growth

**Robotic Motion Planning:**
- **ZYZ singularity proximity analysis** providing mathematical framework for safe orientation control
- **Regrasping optimization theory** balancing manipulation efficiency with kinematic safety constraints
- **Real-time planning algorithms** achieving optimal trade-off between computational overhead and motion quality

**Human-Robot Interaction:**
- **Multi-modal safety architecture** integrating voice, visual, force, and emergency stop systems
- **Cultural and linguistic adaptation** for industrial robotics in non-English speaking environments
- **Trust and reliability metrics** for collaborative manufacturing environments

### 8.3 Current Limitations and Research Challenges

#### 8.3.1 Technical Limitations

**Environmental Constraints:**
- **Controlled laboratory conditions**: Current evaluation limited to consistent lighting, clean surfaces, static environment
- **Limited object diversity**: System currently handles 7 specific Soma Cube pieces with known geometric properties
- **Single robot deployment**: Multi-robot coordination and human-robot collaboration remain future extensions

**Performance Limitations:**
- **75% success rate**: State-of-the-art, but significant improvement opportunities exist for industrial adoption
- **Assembly complexity**: Current system limited to 27 unit cubes; scalability to larger assemblies uncertain
- **Real-time requirements**: 100ms control cycles may be insufficient for high-speed manufacturing applications

#### 8.3.2 Methodological Challenges

**Generalization Capabilities:**
- **Domain specificity**: Legal-action masking and reward functions tailored to polycube assembly
- **Transfer learning limitations**: Performance on fundamentally different assembly tasks uncertain
- **Sim-to-real gap**: 9% performance degradation still indicates room for improvement

### 8.4 Future Research Directions

#### 6.4.1 Short-Term Extensions (6-12 months)

**Enhanced Robustness and Generalization:**

1. **Environmental Adaptation**
   - **Dynamic lighting compensation**: Adaptive vision algorithms handling 200-2000 lux variations
   - **Surface material handling**: Extension to various surface textures, colors, and reflectance properties  
   - **Dust and wear simulation**: Domain randomization including component aging and environmental contamination
   - **Target**: Achieve 80%+ success rate across diverse environmental conditions

2. **Assembly Complexity Scaling**
   - **Extended piece sets**: 15-20 polycube pieces with arbitrary geometric properties
   - **Multi-stage assemblies**: Sequential assembly of multiple Soma Cubes or mixed puzzle types
   - **Dynamic objectives**: Real-time assembly target modifications during execution
   - **Target**: Demonstrate scalability to 50+ component assemblies

3. **Performance Optimization**
   - **Sub-50ms control cycles**: Real-time optimization for high-speed manufacturing
   - **Memory footprint reduction**: Embedded system deployment with <1GB memory requirements
   - **Edge computing integration**: Distributed processing across multiple computational units
   - **Target**: 20 Hz control frequency with 95%+ deadline compliance

#### 6.4.2 Medium-Term Research Directions (1-3 years)

**Advanced Learning and Generalization:**

1. **Foundation Model Integration**
   - **3D understanding models**: Integration with NeRF, PointNet++, or diffusion-based 3D representations
   - **Zero-shot generalization**: Assembly of previously unseen objects without additional training
   - **Multi-modal learning**: Unified visual-tactile-auditory learning representations
   - **Target**: 70%+ success rate on novel object assemblies without retraining

2. **Hierarchical and Meta-Learning**
   - **Compositional reasoning**: Learning reusable assembly primitives and composition rules
   - **Few-shot adaptation**: Rapid learning of new assembly tasks from minimal demonstrations  
   - **Transfer learning**: Cross-domain knowledge transfer between different assembly domains
   - **Target**: <1000 training samples for new assembly task adaptation

3. **Advanced Safety and Human Collaboration**
   - **Predictive safety**: Anticipatory hazard detection and prevention using multi-step prediction
   - **Adaptive collaboration**: Dynamic role allocation between human and robot based on task complexity
   - **Intention recognition**: Understanding human assembly intentions for proactive assistance
   - **Target**: Enable seamless human-robot collaborative assembly in production environments

#### 6.4.3 Long-Term Vision (3-10 years)

**Revolutionary Manufacturing Capabilities:**

1. **Autonomous Manufacturing Systems**
   - **Self-configuring assembly lines**: Robots autonomously adapting to new product designs
   - **Supply chain integration**: End-to-end automated production from raw materials to finished products
   - **Quality assurance automation**: Integrated inspection, testing, and correction capabilities
   - **Vision**: Fully autonomous factories requiring minimal human supervision

2. **Cognitive Manufacturing Intelligence**
   - **Causal reasoning**: Understanding underlying physics and engineering principles in assembly tasks
   - **Creative problem solving**: Novel solution discovery for previously unsolvable assembly challenges
   - **Continuous learning**: Perpetual skill acquisition and improvement throughout deployment lifecycle
   - **Vision**: Manufacturing systems with human-level spatial reasoning and problem-solving capabilities

3. **Societal Impact and Ethical Considerations**
   - **Workforce transformation**: Collaborative systems augmenting rather than replacing human workers
   - **Sustainability optimization**: Minimal waste, energy-efficient, and environmentally conscious manufacturing
   - **Global accessibility**: Democratization of advanced manufacturing capabilities worldwide
   - **Vision**: Technology contributing to sustainable, equitable, and beneficial industrial development

### 6.5 Methodological Framework for Future Research

#### 6.5.1 Systematic Research Roadmap

**Phase 1: Foundation Strengthening (Year 1)**
- Environmental robustness validation across diverse industrial conditions
- Comprehensive failure mode analysis and mitigation strategy development  
- Performance optimization for real-time industrial deployment requirements
- **Success Metrics**: 85% success rate, 50ms control cycles, <1GB memory usage

**Phase 2: Capability Expansion (Years 2-3)**
- Multi-object assembly systems with complex geometric relationships
- Human-robot collaborative manufacturing with adaptive role allocation
- Cross-domain transfer learning validation across different assembly domains
- **Success Metrics**: 15+ object assemblies, seamless human collaboration, 90% transfer efficiency

**Phase 3: Intelligence Augmentation (Years 4-5)**
- Foundation model integration for zero-shot generalization capabilities
- Causal reasoning and creative problem-solving in manufacturing contexts
- Autonomous system reconfiguration and adaptive manufacturing processes
- **Success Metrics**: Novel object handling, autonomous reconfiguration, creative solution discovery

#### 6.5.2 Validation and Deployment Framework

**Academic Validation:**
- Peer review through top-tier robotics and AI conferences (ICRA, IROS, NeurIPS)
- Open-source implementation enabling reproducible research across institutions
- Benchmark dataset creation for standardized comparative evaluation
- Collaborative research initiatives with leading robotics laboratories worldwide

**Industrial Validation:**
- Pilot deployments in automotive, electronics, and pharmaceutical manufacturing
- Performance evaluation under real production constraints and quality requirements  
- Economic impact assessment including ROI, productivity improvement, and cost reduction analysis
- Safety certification compliance with international collaborative robotics standards

**Societal Impact Assessment:**
- Workforce impact studies including job creation, skill requirement evolution, and training needs
- Environmental sustainability analysis of automated manufacturing systems
- Ethical considerations regarding autonomous decision-making in industrial contexts
- Policy recommendation development for responsible deployment of advanced manufacturing robotics

### 6.6 Final Reflections and Broader Impact

This research represents a significant milestone in the evolution of intelligent collaborative robotics, demonstrating that sophisticated spatial reasoning tasks previously requiring human expertise can be successfully automated while maintaining safety, efficiency, and reliability standards suitable for industrial deployment.

#### 6.6.1 Technological Legacy

The methodologies developed in this workâ€”legal-action masking for constraint-aware learning, proximity index-based singularity avoidance, and comprehensive sim-to-real transferâ€”establish reusable frameworks applicable across diverse robotic manipulation domains. The integration of classical robotics principles with modern machine learning approaches provides a template for future research bridging theoretical advances with practical deployment requirements.

#### 6.6.2 Industrial Transformation Potential

Our validated 75% success rate, combined with comprehensive safety verification and real-time performance capabilities, positions this technology for practical industrial adoption. The economic implications are substantial: reducing assembly time by 18% while maintaining zero-incident safety records can significantly impact manufacturing productivity, quality, and worker safety across multiple industries.

#### 6.6.3 Educational and Research Impact

The comprehensive methodology presentedâ€”encompassing reinforcement learning, motion planning, computer vision, human-robot interaction, and system integrationâ€”provides an exemplar for interdisciplinary robotics research. The open challenges identified and systematic research roadmap proposed can guide academic research priorities and industrial R&D investments for the coming decade.

#### 6.6.4 Philosophical Implications

Perhaps most significantly, this research demonstrates that the future of manufacturing lies not in replacing human workers with robots, but in creating intelligent systems capable of seamless collaboration with human expertise. The integration of Korean speech recognition, comprehensive safety systems, and adaptive behavior represents a step toward culturally-sensitive, human-centered automation that augments rather than supplants human capabilities.

> *"In the convergence of artificial intelligence and mechanical precision, we find not merely automated assembly, but the emergence of collaborative intelligence that honors both human insight and computational capability. The future of manufacturing is not human versus machine, but human with machineâ€”each contributing irreplaceable strengths toward our common technological advancement."*

---

## Acknowledgments

We extend our profound gratitude to the **K-Digital Training Program** for providing the platform, resources, and mentorship essential to this research. Special appreciation to **Doosan Robotics** for collaborative robot platform access and technical support, and to all team members who contributed expertise across multiple technical domains.

This research was conducted with support from the **Ministry of Education, South Korea** through the K-Digital Training initiative, representing a collaborative effort between academic research and industrial application development.

---

## References

1. Mnih, V., et al. "Human-level control through deep reinforcement learning." *Nature* 518.7540 (2015): 529-533.

2. Van Hasselt, H., Guez, A., & Silver, D. "Deep reinforcement learning with double q-learning." *Proceedings of the AAAI Conference on Artificial Intelligence* Vol. 30. No. 1. 2016.

3. Wang, Z., et al. "Dueling network architectures for deep reinforcement learning." *International Conference on Machine Learning* PMLR, 2016.

4. Schaul, T., et al. "Prioritized experience replay." *arXiv preprint arXiv:1511.05952* (2015).

5. Tobin, J., et al. "Domain randomization for transferring deep neural networks from simulation to the real world." *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)* IEEE, 2017.

6. Levine, S., et al. "End-to-end training of deep visuomotor policies." *The Journal of Machine Learning Research* 17.1 (2016): 1334-1373.

7. Kalashnikov, D., et al. "Scalable deep reinforcement learning for vision-based robotic manipulation." *Conference on Robot Learning* PMLR, 2018.

8. Nachum, O., et al. "Near-optimal representation learning for hierarchical reinforcement learning." *arXiv preprint arXiv:1810.01257* (2018).

9. Huang, S., et al. "Masked reinforcement learning for invalid action elimination in video games." *Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment* Vol. 18. No. 1. 2022.

10. Bertram, D., et al. "Efficient geometric reasoning for 3D assembly puzzles." *Computational Intelligence* 24.4 (2008): 336-353.

11. Li, J., et al. "Learning to assemble: A deep reinforcement learning approach for 2D puzzle assembly." *2019 International Conference on Robotics and Automation (ICRA)* IEEE, 2019.

12. Craig, J.J. "Introduction to robotics: mechanics and control." *Pearson/Prentice Hall* Upper Saddle River, NJ, USA, 2005.

13. Siciliano, B., & Khatib, O. (Eds.). "Springer handbook of robotics." *Springer* (2016).

14. Kavraki, L.E., et al. "Probabilistic roadmaps for path planning in high-dimensional configuration spaces." *IEEE Transactions on Robotics and Automation* 12.4 (1996): 566-580.

15. Schulman, J., et al. "Finding locally optimal, collision-free trajectories with sequential convex optimization." *Robotics: Science and Systems* Vol. 9. No. 1. 2013.

16. Radford, A., et al. "Robust speech recognition via large-scale weak supervision." *arXiv preprint arXiv:2212.04356* (2022).

17. Thomaz, A.L., & Breazeal, C. "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance." *Proceedings of the 21st National Conference on Artificial Intelligence* Vol. 1. 2006.

18. Silver, D., et al. "Mastering the game of Go with deep neural networks and tree search." *Nature* 529.7587 (2016): 484-489.

19. Vinyals, O., et al. "Grandmaster level in StarCraft II using multi-agent reinforcement learning." *Nature* 575.7782 (2019): 350-354.

20. Wurman, P.R., et al. "Outracing champion Gran Turismo drivers with deep reinforcement learning." *Nature* 602.7896 (2022): 223-228.

---

## Appendix: Technical Implementation Details

### A.1 Hardware Specifications

**Doosan M0609 Collaborative Robot:**
- **Payload**: 6kg maximum
- **Reach**: 900mm spherical workspace  
- **Repeatability**: Â±0.05mm positioning accuracy
- **Joint Configuration**: 6-DOF anthropomorphic design
- **Safety Features**: Built-in collision detection, force limiting

**OnRobot RG2 Gripper:**
- **Stroke Length**: 110mm maximum opening
- **Grip Force**: 3-40N (configurable)
- **Force Sensing**: Built-in 0.1N resolution
- **Control Interface**: Modbus RTU communication

**Intel RealSense D435i Camera:**
- **RGB Resolution**: 1920Ã—1080 @ 30fps
- **Depth Range**: 0.1-10m operational distance
- **Depth Technology**: Active IR stereo with IMU
- **Interface**: USB 3.0 connectivity

### A.2 Software Architecture Components

**ROS2 Humble Distribution:**
```bash
# Core ROS2 packages
sudo apt install ros-humble-desktop-full
sudo apt install ros-humble-moveit
sudo apt install ros-humble-navigation2
sudo apt install ros-humble-tf2-tools
```

**Python Dependencies:**
```bash
# Machine Learning Framework
pip install torch==1.13.1+cu117
pip install ultralytics==8.0.196
pip install filterpy==1.4.5

# Robotics Libraries  
pip install modern_robotics==1.0.0
pip install scipy==1.9.3
pip install numpy==1.24.3

# Computer Vision
pip install opencv-python==4.8.1.78
pip install realsense2_camera
```

**Unity Integration:**
- **Unity Version**: 2022.3 LTS
- **ROS-Unity Bridge**: Custom TCP socket implementation
- **Rendering Pipeline**: Universal Render Pipeline (URP)
- **Point Cloud Visualization**: Custom shader for 300k+ points at 30fps

### A.3 Network Architecture Configuration

**Legal-Action Masked DQN Structure:**
```python
class LegalActionMaskedDQN(nn.Module):
    def __init__(self, state_dim=36, orientation_classes=116, position_classes=27):
        super().__init__()
        
        # Feature extraction layers
        self.feature_net = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256), 
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        # Dual-head architecture
        self.orientation_head = nn.Linear(128, orientation_classes)
        self.position_head = nn.Linear(128, position_classes)
    
    def forward(self, state, legal_mask=None):
        features = self.feature_net(state)
        
        q_orientation = self.orientation_head(features)
        q_position = self.position_head(features)
        
        # Apply legal action masking
        if legal_mask is not None:
            q_orientation = q_orientation.masked_fill(~legal_mask[:, :116], float('-inf'))
            q_position = q_position.masked_fill(~legal_mask[:, 116:], float('-inf'))
        
        return q_orientation, q_position
```

### A.4 ZYZ Singularity Guard Implementation

**Proximity Index Calculation:**
```python
def calculate_proximity_index(beta):
    """
    Calculate proximity index for ZYZ Euler angle beta
    PI(beta) = 1 - |cos(beta)|
    """
    return 1.0 - abs(np.cos(beta))

def is_near_singularity(beta, threshold=0.1):
    """
    Check if configuration is near ZYZ singularity
    """
    pi = calculate_proximity_index(beta)
    return pi < threshold or abs(abs(beta) - np.pi/2) < threshold

def safe_regrasp_sequence(current_pose, target_pose, threshold=0.1):
    """
    Generate safe regrasp sequence avoiding singularities
    """
    # Extract ZYZ angles
    alpha_t, beta_t, gamma_t = extract_zyz_angles(target_pose)
    
    # Apply singularity guard
    if is_near_singularity(beta_t, threshold):
        beta_t = np.sign(beta_t) * (np.pi/2 - 0.1)  # Clamp to safe region
    
    # Generate motion sequence
    motion_sequence = []
    
    # Calculate rotation difference  
    delta_R = target_pose.rotation @ current_pose.rotation.T
    axis, angle = rotation_matrix_to_axis_angle(delta_R)
    
    if abs(angle) > np.pi/2:  # Large rotation - decompose
        # Split into safe increments
        n_steps = int(np.ceil(abs(angle) / (np.pi/2)))
        for i in range(n_steps):
            partial_angle = angle * (i + 1) / n_steps
            partial_rotation = axis_angle_to_rotation_matrix(axis, partial_angle)
            
            motion_sequence.append({
                'type': 'rotate',
                'rotation': partial_rotation,
                'requires_regrasp': i < n_steps - 1
            })
    else:  # Safe direct rotation
        motion_sequence.append({
            'type': 'rotate', 
            'rotation': delta_R,
            'requires_regrasp': False
        })
    
    return motion_sequence
```

This comprehensive implementation provides all necessary technical details for reproducing and extending the research presented in this paper.